{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import gc\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import wandb\n",
    "from hypnettorch.data import FashionMNISTData, MNISTData\n",
    "from hypnettorch.data.special.split_mnist import get_split_mnist_handlers\n",
    "from hypnettorch.data.special.split_cifar import get_split_cifar_handlers\n",
    "from hypnettorch.mnets import LeNet, ZenkeNet, ResNet\n",
    "from hypnettorch.hnets import HMLP, StructuredHMLP, ChunkedHMLP\n",
    "\n",
    "from utils.data import get_mnist_data_loaders, get_emnist_data_loaders, randomize_targets, select_from_classes, get_data_handlers\n",
    "from utils.visualization import show_imgs, get_model_dot\n",
    "from utils.others import measure_alloc_mem, count_parameters\n",
    "from utils.timing import func_timer\n",
    "from utils.metrics import get_accuracy, calc_accuracy, print_arch_summary\n",
    "from utils.hypnettorch_utils import correct_param_shapes, calc_delta_theta, get_reg_loss_for_cond, get_reg_loss, \\\n",
    "    infer, print_stats, print_metrics, clip_grads, take_training_step, init_hnet_unconditionals, remove_hnet_uncondtionals, \\\n",
    "    validate_cells_training_inputs, train_cells\n",
    "from utils.models import get_target_nets, get_hnets, create_tree\n",
    "from single_hypernet import finish_arch_config, get_arch_config, get_config\n",
    "# from main import get_config\n",
    "from configs.hypercl_zenke_splitcifar100 import get_config\n",
    "\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "torch.set_printoptions(precision=3, linewidth=180)\n",
    "%env \"WANDB_NOTEBOOK_NAME\" \"main.ipynb\"\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"num_cells\": 2,\n",
    "    \"epochs\": 10,\n",
    "    # \"max_minibatches_per_epoch\": 1200,\n",
    "    \"max_minibatches_per_epoch\": None,\n",
    "    # \"phases\": [\"hnet->solver\", \"hnet->hnet->solver\", \"hnet->solver\", \"hnet->hnet->solver\", \"hnet->solver\", \"hnet->hnet->solver\"],\n",
    "    \"phases\": [\"hnet->solver\", \"hnet->hnet->solver\"],\n",
    "    \"n_training_iters_solver\": 14,\n",
    "    \"n_training_iters_hnet\": 6,\n",
    "    \"data\": {\n",
    "        # \"name\": \"mnist|fmnist\",\n",
    "        # \"name\": \"splitmnist\",\n",
    "        \"in_shape\": [28, 28, 1],\n",
    "        \"batch_size\": 32,\n",
    "        \"data_dir\": \"data_tmp\",\n",
    "        \"num_tasks\": 5,\n",
    "        \"num_classes_per_task\": 2,\n",
    "        \"validation_size\": 0,\n",
    "    },\n",
    "    \"solver\": {\n",
    "        \"use\": \"lenet\",\n",
    "        \"lenet\": {\n",
    "            \"arch\": \"mnist_large\",\n",
    "            \"no_weights\": True,\n",
    "        },\n",
    "        \"zenkenet\": {\n",
    "            \"arch\": \"cifar\",\n",
    "            \"no_weights\": True,\n",
    "            \"dropout_rate\": 0.15,\n",
    "        },\n",
    "        \"resnet\": {\n",
    "            \"n\": 5,\n",
    "            \"k\": 1,\n",
    "            \"use_bias\": True,\n",
    "            \"no_weights\": True,\n",
    "        },\n",
    "    },\n",
    "    \"hnet\": {\n",
    "        \"model\": {\n",
    "            # \"layers\": [100, 100],\n",
    "            \"layers\": [25,25],\n",
    "            \"dropout_rate\": -1, # hnet doesn't get images -> need to be added to resnet\n",
    "            \"chunk_emb_size\": 80,\n",
    "            \"chunk_size\": 60_000, # 8000\n",
    "            \"num_cond_embs\": None, # specified later\n",
    "            \"cond_in_size\": 48,\n",
    "            \"cond_chunk_embs\": True,\n",
    "            \"root_no_uncond_weights\": False,\n",
    "            \"root_no_cond_weights\": False,\n",
    "            \"children_no_uncond_weights\": True,\n",
    "            \"children_no_cond_weights\": False,\n",
    "        },\n",
    "        \"lr\": 1e-3,\n",
    "        \"reg_lr\": 1e-3,\n",
    "        # \"reg_alpha\": 5e-3, # L2 regularization of solvers' parameters\n",
    "        # \"reg_beta\": 8e-2, # regularization against forgetting other contexts (tasks)\n",
    "        \"reg_alpha\": 1e-2, # L2 regularization of solvers' parameters\n",
    "        \"reg_beta\": 1e-3, # regularization against forgetting other contexts (tasks)\n",
    "        \"detach_d_theta\": True,\n",
    "        \"reg_clip_grads_max_norm\": None,\n",
    "        \"reg_clip_grads_max_value\": 1.,\n",
    "    },\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"wandb_logging\": False,\n",
    "}\n",
    "# first config[\"data\"][\"num_tasks\"] embs for generating target nets, last config[\"data\"][\"num_tasks\"] for generating a child hnet\n",
    "config[\"hnet\"][\"model\"][\"num_cond_embs\"] = config[\"data\"][\"num_tasks\"] * 2\n",
    "\n",
    "print(f\"... Running on {config['device']} ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_handlers = get_data_handlers(config=config)\n",
    "# torch.manual_seed(0)\n",
    "# np.random.seed(0)\n",
    "\n",
    "# if config[\"data\"][\"name\"] == \"mnist|fmnist\":\n",
    "#     mnist = MNISTData(config[\"data\"][\"data_dir\"], use_one_hot=True, validation_size=config[\"data\"][\"validation_size\"])\n",
    "#     fmnist = FashionMNISTData(config[\"data\"][\"data_dir\"], use_one_hot=True, validation_size=config[\"data\"][\"validation_size\"])\n",
    "#     data_handlers = [mnist, fmnist]\n",
    "# elif config[\"data\"][\"name\"] == \"splitmnist\":\n",
    "#     data_handlers = get_split_mnist_handlers(config[\"data\"][\"data_dir\"], use_one_hot=True, num_tasks=config[\"data\"][\"num_tasks\"], num_classes_per_task=config[\"data\"][\"num_classes_per_task\"], validation_size=config[\"data\"][\"validation_size\"])\n",
    "# elif config[\"data\"][\"name\"] == \"splitcifar\":\n",
    "#     data_handlers = get_split_cifar_handlers(config[\"data\"][\"data_dir\"], use_one_hot=True, num_tasks=config[\"data\"][\"num_tasks\"], num_classes_per_task=config[\"data\"][\"num_classes_per_task\"], validation_size=config[\"data\"][\"validation_size\"])\n",
    "\n",
    "# assert config[\"data\"][\"num_tasks\"] == len(data_handlers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_nets = get_target_nets(config=config)\n",
    "solver_root = target_nets[0]\n",
    "solver_child = target_nets[1]\n",
    "# torch.manual_seed(0)\n",
    "# np.random.seed(0)\n",
    "# # target networks (solvers)\n",
    "# if config[\"solver\"][\"use\"] == \"lenet\":\n",
    "#     solver_child = LeNet(in_shape=data_handlers[0].in_shape, num_classes=config[\"data\"][\"num_classes_per_task\"], **config[\"solver\"][\"lenet\"]).to(config[\"device\"])\n",
    "#     solver_root = LeNet(in_shape=data_handlers[0].in_shape, num_classes=config[\"data\"][\"num_classes_per_task\"], **config[\"solver\"][\"lenet\"]).to(config[\"device\"])\n",
    "# elif config[\"solver\"][\"use\"] == \"zenkenet\":\n",
    "#     solver_child = ZenkeNet(in_shape=data_handlers[0].in_shape, num_classes=config[\"data\"][\"num_classes_per_task\"], **config[\"solver\"][\"zenkenet\"]).to(config[\"device\"])\n",
    "#     solver_root = ZenkeNet(in_shape=data_handlers[0].in_shape, num_classes=config[\"data\"][\"num_classes_per_task\"], **config[\"solver\"][\"zenkenet\"]).to(config[\"device\"])\n",
    "# elif config[\"solver\"][\"use\"] == \"resnet\":\n",
    "#     solver_child = ResNet(in_shape=data_handlers[0].in_shape, num_classes=config[\"data\"][\"num_classes_per_task\"], **config[\"solver\"][\"resnet\"]).to(config[\"device\"])\n",
    "#     solver_root = ResNet(in_shape=data_handlers[0].in_shape, num_classes=config[\"data\"][\"num_classes_per_task\"], **config[\"solver\"][\"resnet\"]).to(config[\"device\"])\n",
    "# else:\n",
    "#     raise ValueError(f\"Unknown solver: {config['solver']['use']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hnets = get_hnets(config=config, target_nets_shapes=[-1, solver_child.param_shapes])\n",
    "hnet_root = hnets[0]\n",
    "hnet_child = hnets[1]\n",
    "hnet_root_optim = torch.optim.Adam(hnet_root.parameters(), lr=config[\"hnet\"][\"lr\"])\n",
    "hnet_child_optim = torch.optim.Adam(hnet_child.parameters(), lr=config[\"hnet\"][\"lr\"])\n",
    "\n",
    "arch = [(\"solver_child\", solver_child), (\"solver_root\", solver_root), (\"hnet_child\", hnet_child), (\"hnet_root\", hnet_root)]\n",
    "print_arch_summary(arch)\n",
    "\n",
    "# torch.manual_seed(0)\n",
    "# np.random.seed(0)\n",
    "\n",
    "# hnet_child = ChunkedHMLP(\n",
    "#     solver_child.param_shapes,\n",
    "#     layers=config[\"hnet\"][\"model\"][\"layers\"],\n",
    "#     chunk_size=config[\"hnet\"][\"chunk_size\"],\n",
    "#     chunk_emb_size=config[\"hnet\"][\"chunk_emb_size\"],\n",
    "#     cond_chunk_embs=config[\"hnet\"][\"cond_chunk_embs\"],\n",
    "#     cond_in_size=config[\"hnet\"][\"cond_in_size\"],\n",
    "#     num_cond_embs=config[\"data\"][\"num_tasks\"] * 2, # num_tasks * 2 for child hypernetwork\n",
    "#     no_uncond_weights=True,\n",
    "#     no_cond_weights=False,\n",
    "# ).to(config[\"device\"])\n",
    "# hnet_child_optim = torch.optim.Adam(hnet_child.parameters(), lr=config[\"hnet\"][\"lr\"])\n",
    "\n",
    "# hnet_root = ChunkedHMLP(\n",
    "#     hnet_child.unconditional_param_shapes,\n",
    "#     layers=config[\"hnet\"][\"model\"][\"layers\"],\n",
    "#     dropout_rate=config[\"hnet\"][\"model\"][\"dropout_rate\"], # only for the root hypernetwork\n",
    "#     chunk_size=config[\"hnet\"][\"chunk_size\"],\n",
    "#     chunk_emb_size=config[\"hnet\"][\"chunk_emb_size\"],\n",
    "#     cond_chunk_embs=config[\"hnet\"][\"cond_chunk_embs\"],\n",
    "#     cond_in_size=config[\"hnet\"][\"cond_in_size\"],\n",
    "#     num_cond_embs=config[\"data\"][\"num_tasks\"] * 2, # num_tasks * 2 for child hypernetwork\n",
    "#     no_uncond_weights=False,\n",
    "#     no_cond_weights=False,\n",
    "# ).to(config[\"device\"])\n",
    "# # hnet_root.apply_chunked_hyperfan_init(mnet=hnet_child)\n",
    "# hnet_root_optim = torch.optim.Adam(hnet_root.parameters(), lr=config[\"hnet\"][\"lr\"])\n",
    "\n",
    "# print(\"\\nSummary of parameters:\")\n",
    "# max_possible_num_of_maintained_params = 0\n",
    "# num_of_maintained_params = 0\n",
    "# for name, m in [(\"solver_child\", solver_child), (\"solver_root\", solver_root), (\"hnet_child\", hnet_child), (\"hnet_root\", hnet_root)]:\n",
    "#     print(f\"- {name}:\\t{sum(p.numel() for p in m.parameters())}\\t({sum([np.prod(p) for p in m.param_shapes])} possible)\")\n",
    "#     num_of_maintained_params += sum(p.numel() for p in m.parameters())\n",
    "#     max_possible_num_of_maintained_params += sum([np.prod(p) for p in m.param_shapes])\n",
    "# print(f\"---\\nTotal available parameters:\\t{max_possible_num_of_maintained_params}\")\n",
    "# print(f\"Parameters maintained:\\t\\t{num_of_maintained_params}\")\n",
    "# print(f\"-> Coefficient of compression:\\t{(num_of_maintained_params / max_possible_num_of_maintained_params):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"wandb_logging\"]:\n",
    "    wandb_run = wandb.init(\n",
    "        project=\"Hypernets\", entity=\"johnny1188\", config=config, group=config[\"data\"][\"name\"],\n",
    "        tags=[], notes=f\"\"\n",
    "    )\n",
    "    wandb.watch((hnet_root, hnet_child, solver_root, solver_child), log=\"all\", log_freq=100)\n",
    "else:\n",
    "    wandb_run = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training in continual learning setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize configurations\n",
    "hnets_cond_ids = [\n",
    "    {\"hnet->solver\": {\"hnet_root\": task_i, \"hnet_child\": None}, \"hnet->hnet->solver\": {\"hnet_root\": task_i + len(data_handlers), \"hnet_child\": task_i}}\n",
    "    for task_i in range(len(data_handlers))\n",
    "]\n",
    "datasets_for_eval = {d_i: (cond_ids[\"hnet->solver\"][\"hnet_root\"], cond_ids[\"hnet->hnet->solver\"][\"hnet_root\"], cond_ids[\"hnet->hnet->solver\"][\"hnet_child\"], data_handlers[d_i]) for d_i, cond_ids in enumerate(hnets_cond_ids)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continual learning - segmented backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select cond_ids for hypernets\n",
    "# hnet_root_cond_id = hnets_cond_ids[d_i][phase][\"hnet_root\"]\n",
    "# hnet_child_cond_id = hnets_cond_ids[d_i][phase][\"hnet_child\"]\n",
    "\n",
    "# for i in range(10):\n",
    "#     cells = [\n",
    "#         {\n",
    "#             \"hnet\": hnet_child,\n",
    "#             \"solver\": solver_child,\n",
    "#             \"hnet_optim\": hnet_child_optim,\n",
    "#             \"hnet_to_hnet_cond_id\": None,\n",
    "#             \"hnet_to_solver_cond_id\": hnets_cond_ids[d_i][\"hnet->hnet->solver\"][\"hnet_child\"],\n",
    "#             \"hnet_init_theta\": hnet_root(cond_id=hnets_cond_ids[d_i][\"hnet->hnet->solver\"][\"hnet_root\"]),\n",
    "#             \"hnet_prev_params\": None,\n",
    "#             \"hnet_theta_out_target\": None,\n",
    "#             \"n_training_iters_solver\": 200,\n",
    "#             \"n_training_iters_hnet\": 0,\n",
    "#         },\n",
    "#         {\n",
    "#             \"hnet\": hnet_root,\n",
    "#             \"solver\": solver_root,\n",
    "#             \"hnet_optim\": hnet_root_optim,\n",
    "#             \"hnet_to_hnet_cond_id\": hnets_cond_ids[d_i][\"hnet->hnet->solver\"][\"hnet_root\"],\n",
    "#             \"hnet_to_solver_cond_id\": hnets_cond_ids[d_i][\"hnet->solver\"][\"hnet_root\"],\n",
    "#             \"hnet_init_theta\": None,\n",
    "#             \"hnet_prev_params\": hnet_root_prev_params,\n",
    "#             \"hnet_theta_out_target\": None,\n",
    "#             \"n_training_iters_solver\": 0,\n",
    "#             \"n_training_iters_hnet\": 50,\n",
    "#         }\n",
    "#     ]\n",
    "#     train_cells(X, y, cells, config)\n",
    "\n",
    "#     # generate theta and predict\n",
    "#     y_hat, params_solver = infer(X, phase, hnet_parent_cond_id=hnet_root_cond_id, hnet_child_cond_id=hnet_child_cond_id, hnet_parent=hnet_root, hnet_child=hnet_child, solver_parent=solver_root, solver_child=solver_child)\n",
    "\n",
    "#     # solvers' params regularization\n",
    "#     loss_solver_params_reg = sum([p.norm(p=2) for p in params_solver]) / len(params_solver)\n",
    "#     # task loss\n",
    "#     loss_class = loss_fn(y_hat, y)\n",
    "#     print(loss_class.item(), \" \\t\" ,loss_solver_params_reg.item(), \" \\t\", y_hat.var(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "hnet_root_prev_params = None\n",
    "log_step = 0\n",
    "\n",
    "for p_i, phase in enumerate(config[\"phases\"][1:]):\n",
    "    for d_i, data in enumerate(data_handlers[1:]):\n",
    "        d_i = d_i + 1\n",
    "        # save parameters before solving the task for regularization against forgetting\n",
    "        hnet_root_prev_params = [p.detach().clone() for p_idx, p in enumerate(hnet_root.unconditional_params)]\n",
    "        # select cond_ids for hypernets\n",
    "        # hnet_root_cond_id = hnets_cond_ids[d_i][phase][\"hnet_root\"]\n",
    "        # hnet_child_cond_id = hnets_cond_ids[d_i][phase][\"hnet_child\"]\n",
    "        \n",
    "        for epoch in range(config[\"epochs\"]):\n",
    "            for i, (batch_size, X, y) in enumerate(data.train_iterator(config[\"data\"][\"batch_size\"])):\n",
    "                if config[\"max_minibatches_per_epoch\"] is not None and i > config[\"max_minibatches_per_epoch\"]:\n",
    "                    break\n",
    "\n",
    "                X = data.input_to_torch_tensor(X, config[\"device\"], mode=\"train\")\n",
    "                y = data.output_to_torch_tensor(y, config[\"device\"], mode=\"train\")\n",
    "\n",
    "                if phase == \"hnet->solver\":\n",
    "                    cells = [\n",
    "                        {\n",
    "                            \"hnet\": hnet_root,\n",
    "                            \"solver\": solver_root,\n",
    "                            \"hnet_optim\": hnet_root_optim,\n",
    "                            \"hnet_to_hnet_cond_id\": None,\n",
    "                            \"hnet_to_solver_cond_id\": hnets_cond_ids[d_i][\"hnet->solver\"][\"hnet_root\"],\n",
    "                            \"hnet_init_theta\": None,\n",
    "                            \"hnet_prev_params\": hnet_root_prev_params,\n",
    "                            \"hnet_theta_out_target\": None,\n",
    "                            # \"n_training_iters_solver\": config[\"n_training_iters_solver\"],\n",
    "                            \"n_training_iters_solver\": 1,\n",
    "                            \"n_training_iters_hnet\": 0,\n",
    "                        }\n",
    "                    ]\n",
    "                elif phase == \"hnet->hnet->solver\":\n",
    "                    cells = [\n",
    "                        {\n",
    "                            \"hnet\": hnet_child,\n",
    "                            \"solver\": solver_child,\n",
    "                            \"hnet_optim\": hnet_child_optim,\n",
    "                            \"hnet_to_hnet_cond_id\": None,\n",
    "                            \"hnet_to_solver_cond_id\": hnets_cond_ids[d_i][\"hnet->hnet->solver\"][\"hnet_child\"],\n",
    "                            \"hnet_init_theta\": hnet_root(cond_id=hnets_cond_ids[d_i][\"hnet->hnet->solver\"][\"hnet_root\"]),\n",
    "                            \"hnet_prev_params\": None, # TODO: would those also regularize the root hypernet?\n",
    "                            \"hnet_theta_out_target\": None,\n",
    "                            \"n_training_iters_solver\": config[\"n_training_iters_solver\"],\n",
    "                            \"n_training_iters_hnet\": 0,\n",
    "                        },\n",
    "                        {\n",
    "                            \"hnet\": hnet_root,\n",
    "                            \"solver\": solver_root,\n",
    "                            \"hnet_optim\": hnet_root_optim,\n",
    "                            \"hnet_to_hnet_cond_id\": hnets_cond_ids[d_i][\"hnet->hnet->solver\"][\"hnet_root\"],\n",
    "                            \"hnet_to_solver_cond_id\": hnets_cond_ids[d_i][\"hnet->solver\"][\"hnet_root\"],\n",
    "                            \"hnet_init_theta\": None,\n",
    "                            \"hnet_prev_params\": hnet_root_prev_params,\n",
    "                            \"hnet_theta_out_target\": None, # will get set during the train_cells() call\n",
    "                            # \"n_training_iters_solver\": config[\"n_training_iters_hnet\"], # root cell shouldn't take more steps on task than on hnet\n",
    "                            \"n_training_iters_solver\": 0, # root cell shouldn't take more steps on task than on hnet\n",
    "                            \"n_training_iters_hnet\": config[\"n_training_iters_hnet\"],\n",
    "                        }\n",
    "                    ]\n",
    "\n",
    "                validate_cells_training_inputs(X, y, cells, config)\n",
    "                stats = train_cells(X, y, cells, config, [])\n",
    "                # clear_output(wait=True)\n",
    "                if i % 5 == 4:\n",
    "                    print_metrics(\n",
    "                        datasets_for_eval, config, hnet_root, hnet_child, solver_root, solver_child,\n",
    "                        prefix=f\"[{p_i + 1}:{phase} | {d_i}/{len(data_handlers) - 1} | {epoch + 1}/{config['epochs']} | {i + 1}]\",\n",
    "                        skip_phases=[],\n",
    "                        wandb_run=wandb_run,\n",
    "                        additional_metrics=None\n",
    "                    )\n",
    "                    print(\".\")\n",
    "                    print_stats(reversed(stats))\n",
    "                    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(\n",
    "    datasets_for_eval, config, hnet_root, hnet_child, solver_root, solver_child,\n",
    "    prefix=f\"[{p_i + 1}:{phase} | {d_i}/{len(data_handlers) - 1} | {epoch + 1}/{config['epochs']} | {i + 1}]\",\n",
    "    skip_phases=[],\n",
    "    wandb_run=wandb_run,\n",
    "    additional_metrics=None\n",
    ")\n",
    "print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continual learning - full backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "hnet_root_prev_params = None\n",
    "log_step = 0\n",
    "phases = config[\"phases\"]\n",
    "\n",
    "for p_i, phase in enumerate(phases):\n",
    "    for d_i, data in enumerate(data_handlers):\n",
    "        # save parameters before solving the task for regularization against forgetting\n",
    "        hnet_root_prev_params = [p.detach().clone() for p_idx, p in enumerate(hnet_root.unconditional_params)]\n",
    "        for epoch in range(config[\"epochs\"]):\n",
    "            for i, (batch_size, X, y) in enumerate(data.train_iterator(config[\"data\"][\"batch_size\"])):\n",
    "                if config[\"max_minibatches_per_epoch\"] is not None and i > config[\"max_minibatches_per_epoch\"]:\n",
    "                    break\n",
    "\n",
    "                X = data.input_to_torch_tensor(X, config[\"device\"], mode=\"train\")\n",
    "                y = data.output_to_torch_tensor(y, config[\"device\"], mode=\"train\")\n",
    "\n",
    "                hnet_root_optim.zero_grad()\n",
    "                hnet_child_optim.zero_grad()\n",
    "\n",
    "                # select cond_ids for hypernets\n",
    "                hnet_root_cond_id = hnets_cond_ids[d_i][phase][\"hnet_root\"]\n",
    "                hnet_child_cond_id = hnets_cond_ids[d_i][phase][\"hnet_child\"]\n",
    "                # generate theta and predict\n",
    "                y_hat, params_solver = infer(X, phase, hnet_parent_cond_id=hnet_root_cond_id, hnet_child_cond_id=hnet_child_cond_id,\n",
    "                    hnet_parent=hnet_root, hnet_child=hnet_child, solver_parent=solver_root, solver_child=solver_child, config=config)\n",
    "                \n",
    "                # solvers' params regularization\n",
    "                loss_solver_params_reg = sum([p.norm(p=2) for p in params_solver]) / len(params_solver)\n",
    "                # task loss\n",
    "                loss_class = loss_fn(y_hat, y)\n",
    "                loss = loss_class + config[\"hnet\"][\"reg_alpha\"] * loss_solver_params_reg\n",
    "                loss.backward(retain_graph=True, create_graph=not config[\"hnet\"][\"detach_d_theta\"])\n",
    "                # gradient clipping\n",
    "                clip_grads([hnet_child, hnet_root], config[\"hnet\"][\"reg_clip_grads_max_norm\"], config[\"hnet\"][\"reg_clip_grads_max_value\"])\n",
    "                \n",
    "                # regularization against forgetting other contexts\n",
    "                loss_reg = config[\"hnet\"][\"reg_beta\"] * get_reg_loss(hnet_root, hnet_root_prev_params, curr_cond_id=hnet_root_cond_id, lr=config[\"hnet\"][\"reg_lr\"], detach_d_theta=config[\"hnet\"][\"detach_d_theta\"])\n",
    "                loss_reg.backward()\n",
    "                clip_grads([hnet_child, hnet_root], config[\"hnet\"][\"reg_clip_grads_max_norm\"], config[\"hnet\"][\"reg_clip_grads_max_value\"])\n",
    "                \n",
    "                hnet_root_optim.step()\n",
    "                hnet_child_optim.step()\n",
    "                hnet_root_optim.zero_grad()\n",
    "                hnet_child_optim.zero_grad()\n",
    "\n",
    "                if i % 100 == 99:\n",
    "                    acc = (y_hat.argmax(dim=-1) == y.argmax(dim=-1)).float().mean() * 100.\n",
    "                    print_metrics(\n",
    "                        datasets_for_eval, config=config, hnet_root=hnet_root, hnet_child=hnet_child, solver_root=solver_root, solver_child=solver_child,\n",
    "                        prefix=f\"[{p_i + 1}:{phase} | {d_i}/{len(data_handlers) - 1} | {epoch + 1}/{config['epochs']} | {i + 1}]\",\n",
    "                        skip_phases=[],\n",
    "                        wandb_run=wandb_run, additional_metrics={\n",
    "                            \"loss_class\": loss_class.item(),\n",
    "                            \"acc_class\": acc,\n",
    "                            \"loss_solver_params_reg\": loss_solver_params_reg.item(),\n",
    "                            \"loss_reg\": loss_reg.item(),\n",
    "                        }\n",
    "                    )\n",
    "                    print(\"---\")\n",
    "                    log_step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training in multitask setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "hnet_root_prev_phase_params = None\n",
    "log_step = 0\n",
    "\n",
    "phases = [\"hnet->solver\", \"hnet->hnet->solver\", \"hnet->solver\", \"hnet->hnet->solver\", \"hnet->solver\", \"hnet->hnet->solver\"]\n",
    "for p_i, phase in enumerate(phases):\n",
    "    print(f\"\\n\\n.... Starting phase {phase} ...\")\n",
    "    if wandb_run is not None:\n",
    "        wandb_run.log({\"Phase\": wandb.Table(columns=[\"phase\", \"step\"], data=[[phase, log_step]])}) # log what phase the training is in\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        for i, ((_, m_X, m_y),(_, f_X, f_y)) in enumerate(zip(mnist.train_iterator(config[\"data\"][\"batch_size\"]), fmnist.train_iterator(config[\"data\"][\"batch_size\"]))):\n",
    "            if i > config[\"max_minibatches_per_epoch\"]:\n",
    "                break\n",
    "\n",
    "            # Mini-batch of MNIST samples\n",
    "            m_X = mnist.input_to_torch_tensor(m_X, config[\"device\"], mode=\"train\")\n",
    "            m_y = mnist.output_to_torch_tensor(m_y, config[\"device\"], mode=\"train\")\n",
    "            # Mini-batch of FashionMNIST samples\n",
    "            f_X = fmnist.input_to_torch_tensor(f_X, config[\"device\"], mode=\"train\")\n",
    "            f_y = fmnist.output_to_torch_tensor(f_y, config[\"device\"], mode=\"train\")\n",
    "\n",
    "            hnet_root_optim.zero_grad()\n",
    "            hnet_child_optim.zero_grad()\n",
    "\n",
    "            # MNIST ------------------------------------------------------------\n",
    "            if phase == \"hnet->solver\":\n",
    "                hnet_root_cond_id = 0\n",
    "                hnet_child_cond_id = None\n",
    "            elif phase == \"hnet->hnet->solver\":\n",
    "                hnet_root_cond_id = 2\n",
    "                hnet_child_cond_id = 0\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown phase {phase}\")\n",
    "            \n",
    "            y_hat, params_solver = infer(m_X, phase, hnet_root_cond_id=hnet_root_cond_id, hnet_child_cond_id=hnet_child_cond_id, hnet_root=hnet_root, hnet_child=hnet_child, solver_root=solver_root, solver_child=solver_child)\n",
    "            \n",
    "            # solvers' params regularization + task loss\n",
    "            m_loss_solver_params_reg = sum([p.norm(p=2) for p in params_solver]) / len(params_solver)\n",
    "            m_loss = loss_fn(y_hat, m_y.max(dim=1)[1]) + config[\"hnet\"][\"reg_alpha\"] * m_loss_solver_params_reg\n",
    "            m_acc = (y_hat.argmax(dim=-1) == m_y.argmax(dim=-1)).float().mean() * 100.\n",
    "            m_loss.backward()\n",
    "            if config[\"hnet\"][\"reg_clip_grads_max_norm\"] is not None:\n",
    "                clip_grads([hnet_child, hnet_root], config[\"hnet\"][\"reg_clip_grads_max_norm\"])\n",
    "            \n",
    "            # regularization against forgetting other contexts\n",
    "            m_loss_reg = config[\"hnet\"][\"reg_beta\"] * get_reg_loss(hnet_root, hnet_root_prev_phase_params, hnet_root_optim, curr_cond_id=hnet_root_cond_id, lr=config[\"hnet\"][\"reg_lr\"], detach_d_theta=False)\n",
    "            m_loss_reg.backward()\n",
    "            if config[\"hnet\"][\"reg_clip_grads_max_norm\"] is not None:\n",
    "                clip_grads([hnet_child, hnet_root], config[\"hnet\"][\"reg_clip_grads_max_norm\"])\n",
    "            \n",
    "            hnet_root_optim.step()\n",
    "            hnet_child_optim.step()\n",
    "            hnet_root_optim.zero_grad()\n",
    "            hnet_child_optim.zero_grad()\n",
    "\n",
    "\n",
    "            # FashionMNIST ------------------------------------------------------------\n",
    "            if phase == \"hnet->solver\":\n",
    "                hnet_root_cond_id = 1\n",
    "                hnet_child_cond_id = None\n",
    "            elif phase == \"hnet->hnet->solver\":\n",
    "                hnet_root_cond_id = 3\n",
    "                hnet_child_cond_id = 1\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown phase {phase}\")\n",
    "            \n",
    "            y_hat, params_solver = infer(f_X, phase, hnet_root_cond_id=hnet_root_cond_id, hnet_child_cond_id=hnet_child_cond_id, hnet_root=hnet_root, hnet_child=hnet_child, solver_root=solver_root, solver_child=solver_child)\n",
    "            \n",
    "            # solvers' params regularization + task loss\n",
    "            f_loss_solver_params_reg = sum([p.norm(p=2) for p in params_solver]) / len(params_solver)\n",
    "            f_loss = loss_fn(y_hat, f_y.max(dim=1)[1]) + config[\"hnet\"][\"reg_alpha\"] * f_loss_solver_params_reg\n",
    "            f_acc = (y_hat.argmax(dim=-1) == f_y.argmax(dim=-1)).float().mean() * 100.\n",
    "            f_loss.backward()\n",
    "            if config[\"hnet\"][\"reg_clip_grads_max_norm\"] is not None:\n",
    "                clip_grads([hnet_child, hnet_root], config[\"hnet\"][\"reg_clip_grads_max_norm\"])\n",
    "            \n",
    "            # regularization against forgetting other contexts\n",
    "            f_loss_reg = config[\"hnet\"][\"reg_beta\"] * get_reg_loss(hnet_root, hnet_root_prev_phase_params, hnet_root_optim, curr_cond_id=hnet_root_cond_id, lr=config[\"hnet\"][\"reg_lr\"], detach_d_theta=False)\n",
    "            f_loss_reg.backward()\n",
    "            if config[\"hnet\"][\"reg_clip_grads_max_norm\"] is not None:\n",
    "                clip_grads([hnet_child, hnet_root], config[\"hnet\"][\"reg_clip_grads_max_norm\"])\n",
    "            \n",
    "            hnet_root_optim.step()\n",
    "            hnet_child_optim.step()\n",
    "            hnet_root_optim.zero_grad()\n",
    "            hnet_child_optim.zero_grad()\n",
    "\n",
    "\n",
    "            if i % 100 == 99:\n",
    "                print_metrics(\n",
    "                    {\"MNIST\": (0, 2, 0, mnist), \"FashionMNIST\": (1, 3, 1, fmnist)},\n",
    "                    hnet_root, hnet_child, solver_root, solver_child,\n",
    "                    prefix=f\"[{phase} | {epoch}/{config['epochs']} | {i + 1}]\\nM: {m_loss_reg:.3f} F: {f_loss_reg:.3f}\",\n",
    "                    # skip_phases=[\"hnet->hnet->solver\"] if p_i == 0 and phase == \"hnet->solver\" else [],\n",
    "                    skip_phases=[],\n",
    "                    wandb_run=wandb_run, additional_metrics={\n",
    "                        \"m_loss_class\": m_loss.item() - config[\"hnet\"][\"reg_alpha\"] * m_loss_solver_params_reg.item(),\n",
    "                        \"f_loss_class\": f_loss.item() - config[\"hnet\"][\"reg_alpha\"] * f_loss_solver_params_reg.item(),\n",
    "                        \"m_acc_class\": m_acc,\n",
    "                        \"f_acc_class\": f_acc,\n",
    "                        \"m_loss_solver_params_reg\": m_loss_solver_params_reg.item(),\n",
    "                        \"f_loss_solver_params_reg\": f_loss_solver_params_reg.item(),\n",
    "                        \"m_loss_reg\": m_loss_reg.item(),\n",
    "                        \"f_loss_reg\": f_loss_reg.item(),\n",
    "                    }\n",
    "                )\n",
    "                print(\"---\")\n",
    "                log_step += 1\n",
    "    hnet_root_prev_phase_params = [p.detach().clone() for p_idx, p in enumerate(hnet_root.unconditional_params)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('vylet')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1aba0afea106d50199ec03ffaadaf3934529de3f3f9deaaa8fc5cd22ec9480e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
