{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import gc\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import wandb\n",
    "from hypnettorch.data import FashionMNISTData, MNISTData\n",
    "from hypnettorch.data.special.split_mnist import get_split_mnist_handlers\n",
    "from hypnettorch.data.special.split_cifar import get_split_cifar_handlers\n",
    "from hypnettorch.mnets import LeNet, ZenkeNet, ResNet\n",
    "from hypnettorch.hnets import HMLP, StructuredHMLP, ChunkedHMLP\n",
    "\n",
    "from utils.data import get_mnist_data_loaders, get_emnist_data_loaders, randomize_targets, select_from_classes\n",
    "from utils.visualization import show_imgs, get_model_dot\n",
    "from utils.others import measure_alloc_mem, count_parameters\n",
    "from utils.timing import func_timer\n",
    "from utils.metrics import get_accuracy, calc_accuracy\n",
    "from utils.hypnettorch_utils import correct_param_shapes, calc_delta_theta, get_reg_loss_for_cond, get_reg_loss, \\\n",
    "    infer, print_stats, print_metrics, clip_grads, take_training_step, init_hnet_unconditionals, remove_hnet_uncondtionals, \\\n",
    "    validate_cells_training_inputs, train_cells\n",
    "\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "torch.set_printoptions(precision=3, linewidth=180)\n",
    "%env \"WANDB_NOTEBOOK_NAME\" \"main.ipynb\"\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"epochs\": 20,\n",
    "    # \"max_minibatches_per_epoch\": 1200,\n",
    "    \"max_minibatches_per_epoch\": None,\n",
    "    # \"phases\": [\"hnet->solver\", \"hnet->hnet->solver\", \"hnet->solver\", \"hnet->hnet->solver\", \"hnet->solver\", \"hnet->hnet->solver\"],\n",
    "    \"phases\": [\"hnet->hnet->solver\"],\n",
    "    \"n_training_iters_solver\": 25,\n",
    "    \"n_training_iters_hnet\": 5,\n",
    "    \"data\": {\n",
    "        # \"name\": \"mnist|fmnist\",\n",
    "        \"name\": \"splitcifar\",\n",
    "        \"batch_size\": 32,\n",
    "        \"data_dir\": \"data_tmp\",\n",
    "        \"num_tasks\": 5,\n",
    "        \"num_classes_per_task\": 2,\n",
    "        \"validation_size\": 0,\n",
    "    },\n",
    "    \"solver\": {\n",
    "        \"use\": \"zenkenet\",\n",
    "        \"lenet\": {\n",
    "            \"arch\": \"mnist_large\",\n",
    "            \"no_weights\": True,\n",
    "        },\n",
    "        \"zenkenet\": {\n",
    "            \"arch\": \"cifar\",\n",
    "            \"no_weights\": True,\n",
    "            \"dropout_rate\": 0.15,\n",
    "        },\n",
    "        \"resnet\": {\n",
    "            \"n\": 5,\n",
    "            \"k\": 1,\n",
    "            \"use_bias\": True,\n",
    "            \"no_weights\": True,\n",
    "        },\n",
    "    },\n",
    "    \"hnet\": {\n",
    "        \"lr\": 1e-3,\n",
    "        \"reg_lr\": 1e-3,\n",
    "        \"model\": {\n",
    "            # \"layers\": [100, 100],\n",
    "            \"layers\": [20,20],\n",
    "            \"dropout_rate\": -1, # hmlp doesn't get images -> need to be added to resnet\n",
    "        },\n",
    "        \"chunk_emb_size\": 80,\n",
    "        # \"chunk_size\": 8000,\n",
    "        \"chunk_size\": 60_000,\n",
    "        \"cond_in_size\": 48,\n",
    "        \"cond_chunk_embs\": True,\n",
    "        # \"reg_alpha\": 5e-3, # L2 regularization of solvers' parameters\n",
    "        # \"reg_beta\": 8e-2, # regularization against forgetting other contexts (tasks)\n",
    "        \"reg_alpha\": 1e-4, # L2 regularization of solvers' parameters\n",
    "        \"reg_beta\": 1e-4, # regularization against forgetting other contexts (tasks)\n",
    "        \"detach_d_theta\": True,\n",
    "        \"reg_clip_grads_max_norm\": None,\n",
    "        \"reg_clip_grads_max_value\": 1.,\n",
    "    },\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"wandb_logging\": False,\n",
    "}\n",
    "\n",
    "print(f\"... Running on {config['device']} ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "if config[\"data\"][\"name\"] == \"mnist|fmnist\":\n",
    "    mnist = MNISTData(config[\"data\"][\"data_dir\"], use_one_hot=True, validation_size=config[\"data\"][\"validation_size\"])\n",
    "    fmnist = FashionMNISTData(config[\"data\"][\"data_dir\"], use_one_hot=True, validation_size=config[\"data\"][\"validation_size\"])\n",
    "    data_handlers = [mnist, fmnist]\n",
    "elif config[\"data\"][\"name\"] == \"splitmnist\":\n",
    "    data_handlers = get_split_mnist_handlers(config[\"data\"][\"data_dir\"], use_one_hot=True, num_tasks=config[\"data\"][\"num_tasks\"], num_classes_per_task=config[\"data\"][\"num_classes_per_task\"], validation_size=config[\"data\"][\"validation_size\"])\n",
    "elif config[\"data\"][\"name\"] == \"splitcifar\":\n",
    "    data_handlers = get_split_cifar_handlers(config[\"data\"][\"data_dir\"], use_one_hot=True, num_tasks=config[\"data\"][\"num_tasks\"], num_classes_per_task=config[\"data\"][\"num_classes_per_task\"], validation_size=config[\"data\"][\"validation_size\"])\n",
    "\n",
    "assert config[\"data\"][\"num_tasks\"] == len(data_handlers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "# target networks (solvers)\n",
    "if config[\"solver\"][\"use\"] == \"lenet\":\n",
    "    solver_child = LeNet(in_shape=data_handlers[0].in_shape, num_classes=config[\"data\"][\"num_classes_per_task\"], **config[\"solver\"][\"lenet\"]).to(config[\"device\"])\n",
    "    solver_root = LeNet(in_shape=data_handlers[0].in_shape, num_classes=config[\"data\"][\"num_classes_per_task\"], **config[\"solver\"][\"lenet\"]).to(config[\"device\"])\n",
    "elif config[\"solver\"][\"use\"] == \"zenkenet\":\n",
    "    solver_child = ZenkeNet(in_shape=data_handlers[0].in_shape, num_classes=config[\"data\"][\"num_classes_per_task\"], **config[\"solver\"][\"zenkenet\"]).to(config[\"device\"])\n",
    "    solver_root = ZenkeNet(in_shape=data_handlers[0].in_shape, num_classes=config[\"data\"][\"num_classes_per_task\"], **config[\"solver\"][\"zenkenet\"]).to(config[\"device\"])\n",
    "elif config[\"solver\"][\"use\"] == \"resnet\":\n",
    "    solver_child = ResNet(in_shape=data_handlers[0].in_shape, num_classes=config[\"data\"][\"num_classes_per_task\"], **config[\"solver\"][\"resnet\"]).to(config[\"device\"])\n",
    "    solver_root = ResNet(in_shape=data_handlers[0].in_shape, num_classes=config[\"data\"][\"num_classes_per_task\"], **config[\"solver\"][\"resnet\"]).to(config[\"device\"])\n",
    "else:\n",
    "    raise ValueError(f\"Unknown solver: {config['solver']['use']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "hnet_child = ChunkedHMLP(\n",
    "    solver_child.param_shapes,\n",
    "    layers=config[\"hnet\"][\"model\"][\"layers\"],\n",
    "    chunk_size=config[\"hnet\"][\"chunk_size\"],\n",
    "    chunk_emb_size=config[\"hnet\"][\"chunk_emb_size\"],\n",
    "    cond_chunk_embs=config[\"hnet\"][\"cond_chunk_embs\"],\n",
    "    cond_in_size=config[\"hnet\"][\"cond_in_size\"],\n",
    "    num_cond_embs=config[\"data\"][\"num_tasks\"] * 2, # num_tasks * 2 for child hypernetwork\n",
    "    no_uncond_weights=True,\n",
    "    no_cond_weights=False,\n",
    ").to(config[\"device\"])\n",
    "hnet_child_optim = torch.optim.Adam(hnet_child.parameters(), lr=config[\"hnet\"][\"lr\"])\n",
    "\n",
    "hnet_root = ChunkedHMLP(\n",
    "    hnet_child.unconditional_param_shapes,\n",
    "    layers=config[\"hnet\"][\"model\"][\"layers\"],\n",
    "    dropout_rate=config[\"hnet\"][\"model\"][\"dropout_rate\"], # only for the root hypernetwork\n",
    "    chunk_size=config[\"hnet\"][\"chunk_size\"],\n",
    "    chunk_emb_size=config[\"hnet\"][\"chunk_emb_size\"],\n",
    "    cond_chunk_embs=config[\"hnet\"][\"cond_chunk_embs\"],\n",
    "    cond_in_size=config[\"hnet\"][\"cond_in_size\"],\n",
    "    num_cond_embs=config[\"data\"][\"num_tasks\"] * 2, # num_tasks * 2 for child hypernetwork\n",
    "    no_uncond_weights=False,\n",
    "    no_cond_weights=False,\n",
    ").to(config[\"device\"])\n",
    "# hnet_root.apply_chunked_hyperfan_init(mnet=hnet_child)\n",
    "hnet_root_optim = torch.optim.Adam(hnet_root.parameters(), lr=config[\"hnet\"][\"lr\"])\n",
    "\n",
    "print(\"\\nSummary of parameters:\")\n",
    "max_possible_num_of_maintained_params = 0\n",
    "num_of_maintained_params = 0\n",
    "for name, m in [(\"solver_child\", solver_child), (\"solver_root\", solver_root), (\"hnet_child\", hnet_child), (\"hnet_root\", hnet_root)]:\n",
    "    print(f\"- {name}:\\t{sum(p.numel() for p in m.parameters())}\\t({sum([np.prod(p) for p in m.param_shapes])} possible)\")\n",
    "    num_of_maintained_params += sum(p.numel() for p in m.parameters())\n",
    "    max_possible_num_of_maintained_params += sum([np.prod(p) for p in m.param_shapes])\n",
    "print(f\"---\\nTotal available parameters:\\t{max_possible_num_of_maintained_params}\")\n",
    "print(f\"Parameters maintained:\\t\\t{num_of_maintained_params}\")\n",
    "print(f\"-> Coefficient of compression:\\t{(num_of_maintained_params / max_possible_num_of_maintained_params):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_param_shapes(solver, params):\n",
    "    \"\"\"Correct the shapes of the parameters for the solver\"\"\"\n",
    "    params_solver = []\n",
    "    src_param_i = 0\n",
    "    src_param_start_idx = 0\n",
    "\n",
    "    for target_param_i, p_shape in enumerate(solver_root.param_shapes):\n",
    "        curr_available_src_params = params[src_param_i].flatten()[src_param_start_idx:].numel()\n",
    "        if curr_available_src_params >= math.prod(p_shape):\n",
    "            params_solver.append(params[src_param_i].flatten()[src_param_start_idx:src_param_start_idx + math.prod(p_shape)].view(p_shape))\n",
    "            src_param_start_idx += math.prod(p_shape)\n",
    "        else:\n",
    "            new_param = torch.zeros(math.prod(p_shape), device=config[\"device\"])\n",
    "            s = 0\n",
    "\n",
    "            while math.prod(p_shape) > s:\n",
    "                curr_available_src_params = params[src_param_i].flatten().numel()\n",
    "                to_add = params[src_param_i].flatten()[src_param_start_idx:min(curr_available_src_params, src_param_start_idx + (math.prod(p_shape) - s))]\n",
    "                new_param[s:s + to_add.numel()] = to_add\n",
    "                s += to_add.numel()\n",
    "\n",
    "                if s < math.prod(p_shape):\n",
    "                    src_param_i += 1\n",
    "                    src_param_start_idx = 0\n",
    "                else:\n",
    "                    src_param_start_idx += to_add.numel()\n",
    "\n",
    "            params_solver.append(new_param.view(p_shape))\n",
    "    return params_solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_delta_theta(hnet, lr, clip_delta=True, detach=False):\n",
    "    ret = []\n",
    "    for p in hnet.internal_params:\n",
    "        if p.grad is None:\n",
    "            ret.append(None)\n",
    "            continue\n",
    "        if detach:\n",
    "            ret.append(-lr * p.grad.detach().clone())\n",
    "        else:\n",
    "            ret.append(-lr * p.grad.clone())\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reg_loss_for_cond(hnet, hnet_prev_params, lr, reg_cond_id, detach_d_theta=False):\n",
    "    # prepare targets (theta for child nets predicted by previous hnet)\n",
    "    hnet_mode = hnet.training\n",
    "    hnet.eval()\n",
    "    with torch.no_grad():\n",
    "        theta_child_target = hnet(cond_id=reg_cond_id, weights={\"uncond_weights\": hnet_prev_params} if hnet_prev_params is not None else None)\n",
    "    # detaching target below is important!\n",
    "    theta_child_target = torch.cat([p.detach().clone().view(-1) for p in theta_child_target])\n",
    "    hnet.train(mode=hnet_mode)\n",
    "    \n",
    "    d_theta = calc_delta_theta(hnet, lr, detach=detach_d_theta)\n",
    "    theta_parent_for_pred = []\n",
    "    for _theta, _d_theta in zip(hnet.internal_params, d_theta):\n",
    "        if _d_theta is None:\n",
    "            theta_parent_for_pred.append(_theta)\n",
    "        else:\n",
    "            theta_parent_for_pred.append(_theta + _d_theta if detach_d_theta is False else _theta + _d_theta.detach())\n",
    "    theta_child_predicted = hnet(cond_id=reg_cond_id, weights=theta_parent_for_pred)\n",
    "    theta_child_predicted = torch.cat([p.view(-1) for p in theta_child_predicted])\n",
    "\n",
    "    return (theta_child_target - theta_child_predicted).pow(2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reg_loss(hnet, hnet_prev_params, curr_cond_id, lr=1e-3, clip_grads_max_norm=1., detach_d_theta=False):\n",
    "    reg_loss = 0\n",
    "    for c_i in range(hnet._num_cond_embs):\n",
    "        if curr_cond_id is not None and c_i == curr_cond_id:\n",
    "            continue\n",
    "        reg_loss += get_reg_loss_for_cond(hnet, hnet_prev_params, lr, c_i, detach_d_theta)\n",
    "    return reg_loss / (hnet._num_cond_embs - (curr_cond_id is not None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(X, scenario, hnet_parent_cond_id, hnet_child_cond_id, hnet_parent, hnet_child, solver_parent, solver_child):\n",
    "    assert scenario != \"hnet->hnet->solver\" or hnet_child_cond_id is not None, f\"Scenario {scenario} requires hnet_child_cond_id to be set\"\n",
    "    \n",
    "    if scenario == \"hnet->solver\":\n",
    "        params_solver = hnet_parent.forward(cond_id=hnet_parent_cond_id) # parent hnet -> theta parent solver\n",
    "        y_hat = solver_parent.forward(X, weights=correct_param_shapes(solver_parent, params_solver))\n",
    "    elif scenario == \"hnet->hnet->solver\":\n",
    "        params_hnet_child = hnet_parent.forward(cond_id=hnet_parent_cond_id) # parent hnet -> theta child hnet (only the unconditional ones) -> solver child\n",
    "        params_solver = hnet_child.forward(cond_id=hnet_child_cond_id, weights=params_hnet_child)\n",
    "        y_hat = solver_child.forward(X, weights=params_solver)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown inference scenario {scenario}\")\n",
    "    return y_hat, params_solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(datasets : dict, hnet_root, hnet_child, solver_root, solver_child, prefix=\"\", skip_phases=[], wandb_run=None, additional_metrics=None):\n",
    "    # set the models to eval mode and return them to their original mode after\n",
    "    ms_modes = []\n",
    "    for m in [hnet_root, hnet_child, solver_root, solver_child]:\n",
    "        ms_modes.append([m, m.training])\n",
    "        m.eval()\n",
    "    wandb_metrics = {}\n",
    "    \n",
    "    print(prefix)\n",
    "    with torch.no_grad():\n",
    "        for data_name, (hnet_root_cond_id_hnet_solver, hnet_root_cond_id_hnet_hnet_solver, hnet_child_cond_id, dataset) in datasets.items():\n",
    "            print(data_name)\n",
    "\n",
    "            # prepare a test batch for calculating loss & getting solver params\n",
    "            X = dataset.input_to_torch_tensor(dataset.get_test_inputs(), config[\"device\"], mode=\"inference\")\n",
    "            y = dataset.output_to_torch_tensor(dataset.get_test_outputs(), config[\"device\"], mode=\"inference\")\n",
    "\n",
    "            hnet_solver_loss, hnet_solver_acc, hnet_hnet_solver_loss, hnet_hnet_solver_acc = np.nan, np.nan, np.nan, np.nan\n",
    "            if \"hnet->solver\" not in skip_phases:\n",
    "                print(\"    hnet->solver\")\n",
    "                y_hat, params_solver = infer(X, \"hnet->solver\", hnet_parent_cond_id=hnet_root_cond_id_hnet_solver, hnet_child_cond_id=None, hnet_parent=hnet_root, hnet_child=hnet_child, solver_parent=solver_root, solver_child=solver_child)\n",
    "                hnet_solver_loss = F.cross_entropy(y_hat, y).item()\n",
    "                hnet_solver_acc = (y_hat.argmax(dim=-1) == y.argmax(dim=-1)).float().mean() * 100.\n",
    "                print(f\"        Loss: {hnet_solver_loss:.3f} | Accuracy: {hnet_solver_acc:.3f}\")\n",
    "            \n",
    "            if \"hnet->hnet->solver\" not in skip_phases:\n",
    "                print(\"    hnet->hnet->solver\")\n",
    "                y_hat, params_solver = infer(X, \"hnet->hnet->solver\", hnet_parent_cond_id=hnet_root_cond_id_hnet_hnet_solver, hnet_child_cond_id=hnet_child_cond_id, hnet_parent=hnet_root, hnet_child=hnet_child, solver_parent=solver_root, solver_child=solver_child)\n",
    "                hnet_hnet_solver_loss = F.cross_entropy(y_hat, y).item()\n",
    "                hnet_hnet_solver_acc = (y_hat.argmax(dim=-1) == y.argmax(dim=-1)).float().mean() * 100.\n",
    "                print(f\"        Loss: {hnet_hnet_solver_loss:.3f} | Accuracy: {hnet_hnet_solver_acc:.3f}\")\n",
    "            \n",
    "            wandb_metrics[str(data_name)] = {\n",
    "                \"h->s loss\": hnet_solver_loss,\n",
    "                \"h->s acc\": hnet_solver_acc,\n",
    "                \"h->h->s loss\": hnet_hnet_solver_loss,\n",
    "                \"h->h->s acc\": hnet_hnet_solver_acc,\n",
    "            }\n",
    "    \n",
    "    if additional_metrics:\n",
    "        wandb_metrics.update(additional_metrics)\n",
    "        for n, v in additional_metrics.items():\n",
    "            print(f\"{n}: {v:.3f}\")\n",
    "\n",
    "    if wandb_run is not None:\n",
    "        wandb_run.log(wandb_metrics)\n",
    "    \n",
    "    for m, mode in ms_modes:\n",
    "        m.train(mode=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(stats):\n",
    "    for c_i, lh in enumerate(stats):\n",
    "        print(f\"{c_i if c_i != 0 else f'{c_i} (root)'}:\")\n",
    "        print('\\n'.join([f'{k:>30}\\t{f\"{v.item():.4f}\" if v.numel() == 1 else v.tolist()}' for k,v in dict(sorted(lh.items())).items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_grads(models, reg_clip_grads_max_norm, reg_clip_grads_max_value):\n",
    "    if reg_clip_grads_max_norm is not None and reg_clip_grads_max_value is not None:\n",
    "        print(\"Warning: both reg_clip_grads_max_norm and reg_clip_grads_max_value are set. Using reg_clip_grads_max_norm.\")\n",
    "    for m in models:\n",
    "        if reg_clip_grads_max_norm is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(m.parameters(), reg_clip_grads_max_norm)\n",
    "        elif reg_clip_grads_max_value is not None:\n",
    "            torch.nn.utils.clip_grad_value_(m.parameters(), reg_clip_grads_max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"wandb_logging\"]:\n",
    "    wandb_run = wandb.init(\n",
    "        project=\"Hypernets\", entity=\"johnny1188\", config=config, group=config[\"data\"][\"name\"],\n",
    "        tags=[], notes=f\"\"\n",
    "    )\n",
    "    wandb.watch((hnet_root, hnet_child, solver_root, solver_child), log=\"all\", log_freq=100)\n",
    "else:\n",
    "    wandb_run = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training in continual learning setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize configurations\n",
    "hnets_cond_ids = [\n",
    "    {\"hnet->solver\": {\"hnet_root\": task_i, \"hnet_child\": None}, \"hnet->hnet->solver\": {\"hnet_root\": task_i + len(data_handlers), \"hnet_child\": task_i}}\n",
    "    for task_i in range(len(data_handlers))\n",
    "]\n",
    "datasets_for_eval = {d_i: (cond_ids[\"hnet->solver\"][\"hnet_root\"], cond_ids[\"hnet->hnet->solver\"][\"hnet_root\"], cond_ids[\"hnet->hnet->solver\"][\"hnet_child\"], data_handlers[d_i]) for d_i, cond_ids in enumerate(hnets_cond_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_training_step(X, y, parent, child, phase, hnet_parent_prev_params, config, loss_fn=F.cross_entropy):\n",
    "    \"\"\"\n",
    "    parent and child structure: tuple (hnet, solver, hnet_optimizer, hnet_cond_id)\n",
    "        child can be None if phase is \"hnet->solver\"\n",
    "    \"\"\"\n",
    "    hnet_parent, solver_parent, hnet_parent_optim, hnet_parent_cond_id = parent\n",
    "    hnet_child, solver_child, hnet_child_optim, hnet_child_cond_id = child\n",
    "    for m in (hnet_parent, solver_parent, hnet_child, solver_child):\n",
    "        if m is not None:\n",
    "            m.train(mode=True)\n",
    "    \n",
    "    hnet_parent_optim.zero_grad()\n",
    "    if hnet_child_optim is not None:\n",
    "        hnet_child_optim.zero_grad()\n",
    "    hnet_parent_optim.zero_grad()\n",
    "    if hnet_child_optim is not None:\n",
    "        hnet_child_optim.zero_grad()\n",
    "\n",
    "    # generate theta and predict\n",
    "    y_hat, params_solver = infer(X, phase, hnet_parent_cond_id=hnet_parent_cond_id, hnet_child_cond_id=hnet_child_cond_id,\n",
    "        hnet_parent=hnet_parent, hnet_child=hnet_child, solver_parent=solver_parent, solver_child=solver_child)\n",
    "    \n",
    "    # task loss\n",
    "    loss_class = loss_fn(y_hat, y)\n",
    "    loss = loss_class\n",
    "    # solvers' params regularization\n",
    "    loss_solver_params_reg = torch.tensor(0., device=config[\"device\"])\n",
    "    if config[\"hnet\"][\"reg_alpha\"] is not None and config[\"hnet\"][\"reg_alpha\"] > 0.:\n",
    "        loss_solver_params_reg = config[\"hnet\"][\"reg_alpha\"] * sum([p.norm(p=2) for p in params_solver]) / len(params_solver)\n",
    "    loss += loss_solver_params_reg\n",
    "    perform_forgetting_reg = config[\"hnet\"][\"reg_beta\"] is not None and config[\"hnet\"][\"reg_beta\"] > 0.\n",
    "    loss.backward(retain_graph=perform_forgetting_reg, create_graph=not config[\"hnet\"][\"detach_d_theta\"])\n",
    "    # gradient clipping\n",
    "    clip_grads([m for m in (hnet_parent, hnet_child) if m is not None], config[\"hnet\"][\"reg_clip_grads_max_norm\"], config[\"hnet\"][\"reg_clip_grads_max_value\"])\n",
    "    \n",
    "    # regularization against forgetting other contexts\n",
    "    loss_reg = torch.tensor(0., device=config[\"device\"])\n",
    "    if config[\"hnet\"][\"reg_beta\"] is not None and config[\"hnet\"][\"reg_beta\"] > 0.:\n",
    "        loss_reg = config[\"hnet\"][\"reg_beta\"] * get_reg_loss(hnet_parent, hnet_parent_prev_params, curr_cond_id=hnet_parent_cond_id, lr=config[\"hnet\"][\"reg_lr\"], detach_d_theta=config[\"hnet\"][\"detach_d_theta\"])\n",
    "        loss_reg.backward()\n",
    "        # gradient clipping\n",
    "        clip_grads([m for m in (hnet_parent, hnet_child) if m is not None], config[\"hnet\"][\"reg_clip_grads_max_norm\"], config[\"hnet\"][\"reg_clip_grads_max_value\"])\n",
    "    \n",
    "    hnet_parent_optim.step()\n",
    "    if hnet_child_optim is not None:\n",
    "        hnet_child_optim.step()\n",
    "    hnet_parent_optim.zero_grad()\n",
    "    if hnet_child_optim is not None:\n",
    "        hnet_child_optim.zero_grad()\n",
    "\n",
    "    return loss_class.detach().clone(), loss_solver_params_reg.detach().clone(), loss_reg.detach().clone(), y_hat.var(dim=0).detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_hnet_unconditionals(hnet, uncond_theta):\n",
    "    assert [s for s in hnet.unconditional_param_shapes] == [list(p.shape) for p in uncond_theta], f\"uncond_theta shapes don't match hnet.unconditional_param_shapes\"\n",
    "    \n",
    "    params_before = [p.clone() for p in hnet.internal_params]\n",
    "    params_final = [None] * len(hnet.param_shapes)\n",
    "    # add conditional params\n",
    "    for p_idx, p in zip(hnet.conditional_param_shapes_ref, hnet.conditional_params):\n",
    "        params_final[p_idx] = p\n",
    "    # add unconditional params\n",
    "    hnet._unconditional_params_ref = hnet.unconditional_param_shapes_ref\n",
    "    for p_idx, p in zip(hnet.unconditional_param_shapes_ref, uncond_theta):\n",
    "        params_final[p_idx] = nn.Parameter(p.detach().clone(), requires_grad=True)\n",
    "    # set internal params\n",
    "    hnet._hnet._internal_params = nn.ParameterList(params_final)\n",
    "    hnet._internal_params = nn.ParameterList(hnet._hnet._internal_params)\n",
    "\n",
    "    return params_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hnet_uncondtionals(hnet, prev_params=None):\n",
    "    # store the unconditional parameters\n",
    "    unconditionals = []\n",
    "    for p_idx in hnet.unconditional_param_shapes_ref:\n",
    "        unconditionals.append(hnet.internal_params[p_idx].detach().clone())\n",
    "    \n",
    "    # restore previous state of parameters\n",
    "    hnet._unconditional_params_ref = None\n",
    "    \n",
    "    hnet._hnet._internal_params = nn.ParameterList([\n",
    "        p for p_idx, p in enumerate(hnet.internal_params) if p_idx not in hnet.unconditional_param_shapes_ref\n",
    "    ])\n",
    "    hnet._internal_params = nn.ParameterList(hnet._hnet._internal_params) # TODO: chunked_mlp_hnet line 201\n",
    "    \n",
    "    # append additional conditional chunk embeddings\n",
    "    if hnet._cemb_shape is not None and prev_params is not None:\n",
    "        for c_i in range(hnet._num_cond_embs):\n",
    "            param_to_add = prev_params[-hnet._num_cond_embs + c_i]\n",
    "            if type(param_to_add) == nn.Parameter:\n",
    "                param_to_add = param_to_add.detach().clone()\n",
    "            elif type(param_to_add) == torch.Tensor:\n",
    "                param_to_add = nn.Parameter(param_to_add.detach().clone(), requires_grad=True)\n",
    "            else:\n",
    "                raise ValueError(f\"prev_params includes a value of type {type(param_to_add)}\")\n",
    "            hnet._internal_params.append(param_to_add)\n",
    "    return unconditionals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_cells_training_inputs(X, y, cells, config):\n",
    "    assert X.shape[0] == y.shape[0], f\"X and y have different number of samples\"\n",
    "    assert y.shape[1] == config[\"data\"][\"num_classes_per_task\"], f\"y has incorrect number of features\"\n",
    "    assert cells[0][\"hnet_to_hnet_cond_id\"] is None and cells[0][\"hnet_theta_out_target\"] is None and cells[0][\"n_training_iters_hnet\"] in (None, 0), \\\n",
    "        f\"The last cell should have no child cells (list of cells sorted from the furthest from the root to the closest to the root)\"\n",
    "    assert cells[-1][\"hnet_init_theta\"] is None, f\"The root cell (last in the cells list) should have no initial theta - its parameters are being learned\"\n",
    "    \n",
    "    for c_i, c in enumerate(cells):\n",
    "        assert set((\"hnet\", \"solver\", \"hnet_optim\", \"hnet_to_hnet_cond_id\", \"hnet_to_solver_cond_id\", \"hnet_init_theta\", \"hnet_prev_params\",\n",
    "            \"hnet_theta_out_target\", \"n_training_iters_solver\", \"n_training_iters_hnet\")).issubset(set(c.keys())), f\"Cell {c_i} is missing some of the required keys\"\n",
    "        if c_i + 1 < len(cells) - 1:\n",
    "            assert sum([np.prod(p) for p in c[\"hnet\"].unconditional_param_shapes]) == cells[c_i + 1][\"hnet\"].num_outputs, \\\n",
    "                f\"Number of outputs of the {c_i + 1}-th cell's hnet should be equal to the number of unconditional parameters of the {c_i}-th cell's hnet\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cells(X, y, cells, config, stats):\n",
    "    \"\"\"\n",
    "    cells: list of dictionaries with the following keys (and corresponding values):\n",
    "        {\n",
    "            \"hnet\", \"solver\", \"hnet_optim\", \"hnet_to_hnet_cond_id\", \"hnet_to_solver_cond_id\", \"hnet_init_theta\", \"hnet_prev_params\",\n",
    "            \"hnet_theta_out_target\", \"n_training_iters_solver\", \"n_training_iters_hnet\"\n",
    "        }\n",
    "        List of cells sorted from the furthest from the root to the closest to the root.\n",
    "    \"\"\"\n",
    "    if len(cells) == 0:\n",
    "        return stats\n",
    "\n",
    "    # pop the first cell\n",
    "    hnet, solver, hnet_optim, hnet_to_hnet_cond_id, hnet_to_solver_cond_id, hnet_init_theta, hnet_prev_params, hnet_theta_out_target, \\\n",
    "        n_training_iters_solver, n_training_iters_hnet = cells.pop(0).values()\n",
    "\n",
    "    # initialize statistics - logging purposes\n",
    "    c_stats = {l:torch.tensor(0.) for l in (\"loss_hnet_hnet\", \"loss_hnet_solver_class\", \"loss_hnet_solver_theta_reg\", \"loss_hnet_forgetting_reg\", \"y_hat_var\")}\n",
    "    \n",
    "    # train the hnet -> solver on the given X, y => create theta target for parent hnet\n",
    "    if n_training_iters_solver is not None and n_training_iters_solver > 0:\n",
    "        if hnet_init_theta is not None: # is None for the root hnet\n",
    "            init_hnet_unconditionals(hnet, hnet_init_theta)\n",
    "            # init optimizer of the initialized  unconditional parameters\n",
    "            hnet_optim = torch.optim.Adam([*hnet.unconditional_params, *hnet.conditional_params], lr=config[\"hnet\"][\"lr\"])\n",
    "        for iter_i in range(n_training_iters_solver):\n",
    "            curr_cell = (hnet, solver, hnet_optim, hnet_to_solver_cond_id)\n",
    "            c_stats[\"loss_hnet_solver_class\"], c_stats[\"loss_hnet_solver_theta_reg\"], c_stats[\"loss_hnet_forgetting_reg\"], c_stats[\"y_hat_var\"] = take_training_step(\n",
    "                X, y, parent=curr_cell, child=(None, None, None, None), phase=\"hnet->solver\",\n",
    "                hnet_parent_prev_params=hnet_prev_params, config=config, loss_fn=F.cross_entropy\n",
    "            )\n",
    "        # set the trained theta as the target for parent hnet\n",
    "        if hnet_init_theta is not None: # is None for the root hnet\n",
    "            cells[0][\"hnet_theta_out_target\"] = remove_hnet_uncondtionals(hnet)\n",
    "    \n",
    "    # train the hnet -> hnet on the given target theta\n",
    "    if n_training_iters_hnet is not None and n_training_iters_hnet > 0:\n",
    "        if hnet_init_theta is not None: # is None for the root hnet\n",
    "            assert len(cells) == 0, \"hnet_init_theta is not None for a non-root hnet\"\n",
    "            init_hnet_unconditionals(hnet, hnet_init_theta)\n",
    "            # init optimizer of the initialized unconditional parameters\n",
    "            hnet_optim = torch.optim.Adam([*hnet.unconditional_params, *hnet.conditional_params], lr=config[\"hnet\"][\"lr\"])\n",
    "        perform_forgetting_reg = config[\"hnet\"][\"reg_beta\"] is not None and config[\"hnet\"][\"reg_beta\"] > 0.\n",
    "        \n",
    "        for iter_i in range(n_training_iters_hnet):\n",
    "            theta_target = torch.cat([p.detach().clone().view(-1) for p in hnet_theta_out_target])\n",
    "            \n",
    "            theta_hat = hnet(cond_id=hnet_to_hnet_cond_id)\n",
    "            theta_hat = torch.cat([p.view(-1) for p in theta_hat])\n",
    "            \n",
    "            loss_hnet_hnet = torch.sqrt(F.mse_loss(theta_hat, theta_target))\n",
    "            # loss_hnet_hnet = (theta_hat - theta_target).pow(2).sum()\n",
    "            loss_hnet_hnet.backward(retain_graph=perform_forgetting_reg, create_graph=not config[\"hnet\"][\"detach_d_theta\"])\n",
    "            # gradient clipping\n",
    "            clip_grads([hnet], config[\"hnet\"][\"reg_clip_grads_max_norm\"], config[\"hnet\"][\"reg_clip_grads_max_value\"])\n",
    "\n",
    "            # regularization against forgetting other contexts\n",
    "            loss_reg = torch.tensor(0., device=config[\"device\"])\n",
    "            if perform_forgetting_reg:\n",
    "                loss_reg = config[\"hnet\"][\"reg_beta\"] * get_reg_loss(hnet, hnet_prev_params, curr_cond_id=hnet_to_hnet_cond_id, lr=config[\"hnet\"][\"reg_lr\"], detach_d_theta=config[\"hnet\"][\"detach_d_theta\"])\n",
    "                loss_reg.backward()\n",
    "                # gradient clipping\n",
    "                clip_grads([hnet], config[\"hnet\"][\"reg_clip_grads_max_norm\"], config[\"hnet\"][\"reg_clip_grads_max_value\"])\n",
    "\n",
    "            hnet_optim.step()\n",
    "            hnet_optim.zero_grad()\n",
    "            c_stats[\"loss_hnet_hnet\"] = loss_hnet_hnet.detach().clone()\n",
    "        # set the trained theta as the target for parent hnet\n",
    "        if hnet_init_theta is not None: # is None for the root hnet\n",
    "            cells[0][\"hnet_theta_out_target\"] = remove_hnet_uncondtionals(hnet)\n",
    "\n",
    "\n",
    "    # one step deeper (onto the parents of the current cell)\n",
    "    stats.append(c_stats)\n",
    "    return train_cells(X, y, cells, config, stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select cond_ids for hypernets\n",
    "# hnet_root_cond_id = hnets_cond_ids[d_i][phase][\"hnet_root\"]\n",
    "# hnet_child_cond_id = hnets_cond_ids[d_i][phase][\"hnet_child\"]\n",
    "\n",
    "# for i in range(10):\n",
    "#     cells = [\n",
    "#         {\n",
    "#             \"hnet\": hnet_child,\n",
    "#             \"solver\": solver_child,\n",
    "#             \"hnet_optim\": hnet_child_optim,\n",
    "#             \"hnet_to_hnet_cond_id\": None,\n",
    "#             \"hnet_to_solver_cond_id\": hnets_cond_ids[d_i][\"hnet->hnet->solver\"][\"hnet_child\"],\n",
    "#             \"hnet_init_theta\": hnet_root(cond_id=hnets_cond_ids[d_i][\"hnet->hnet->solver\"][\"hnet_root\"]),\n",
    "#             \"hnet_prev_params\": None,\n",
    "#             \"hnet_theta_out_target\": None,\n",
    "#             \"n_training_iters_solver\": 200,\n",
    "#             \"n_training_iters_hnet\": 0,\n",
    "#         },\n",
    "#         {\n",
    "#             \"hnet\": hnet_root,\n",
    "#             \"solver\": solver_root,\n",
    "#             \"hnet_optim\": hnet_root_optim,\n",
    "#             \"hnet_to_hnet_cond_id\": hnets_cond_ids[d_i][\"hnet->hnet->solver\"][\"hnet_root\"],\n",
    "#             \"hnet_to_solver_cond_id\": hnets_cond_ids[d_i][\"hnet->solver\"][\"hnet_root\"],\n",
    "#             \"hnet_init_theta\": None,\n",
    "#             \"hnet_prev_params\": hnet_root_prev_params,\n",
    "#             \"hnet_theta_out_target\": None,\n",
    "#             \"n_training_iters_solver\": 0,\n",
    "#             \"n_training_iters_hnet\": 50,\n",
    "#         }\n",
    "#     ]\n",
    "#     train_cells(X, y, cells, config)\n",
    "\n",
    "#     # generate theta and predict\n",
    "#     y_hat, params_solver = infer(X, phase, hnet_parent_cond_id=hnet_root_cond_id, hnet_child_cond_id=hnet_child_cond_id, hnet_parent=hnet_root, hnet_child=hnet_child, solver_parent=solver_root, solver_child=solver_child)\n",
    "\n",
    "#     # solvers' params regularization\n",
    "#     loss_solver_params_reg = sum([p.norm(p=2) for p in params_solver]) / len(params_solver)\n",
    "#     # task loss\n",
    "#     loss_class = loss_fn(y_hat, y)\n",
    "#     print(loss_class.item(), \" \\t\" ,loss_solver_params_reg.item(), \" \\t\", y_hat.var(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continual learning - segmented backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "hnet_root_prev_params = None\n",
    "log_step = 0\n",
    "phases = config[\"phases\"]\n",
    "\n",
    "for p_i, phase in enumerate(phases):\n",
    "    for d_i, data in enumerate(data_handlers):\n",
    "        # save parameters before solving the task for regularization against forgetting\n",
    "        hnet_root_prev_params = [p.detach().clone() for p_idx, p in enumerate(hnet_root.unconditional_params)]\n",
    "        # select cond_ids for hypernets\n",
    "        hnet_root_cond_id = hnets_cond_ids[d_i][phase][\"hnet_root\"]\n",
    "        hnet_child_cond_id = hnets_cond_ids[d_i][phase][\"hnet_child\"]\n",
    "        \n",
    "        for epoch in range(config[\"epochs\"]):\n",
    "            for i, (batch_size, X, y) in enumerate(data.train_iterator(config[\"data\"][\"batch_size\"])):\n",
    "                if config[\"max_minibatches_per_epoch\"] is not None and i > config[\"max_minibatches_per_epoch\"]:\n",
    "                    break\n",
    "\n",
    "                X = data.input_to_torch_tensor(X, config[\"device\"], mode=\"train\")\n",
    "                y = data.output_to_torch_tensor(y, config[\"device\"], mode=\"train\")\n",
    "\n",
    "                if phase == \"hnet->solver\":\n",
    "                    cells = [\n",
    "                        {\n",
    "                            \"hnet\": hnet_root,\n",
    "                            \"solver\": solver_root,\n",
    "                            \"hnet_optim\": hnet_root_optim,\n",
    "                            \"hnet_to_hnet_cond_id\": None,\n",
    "                            \"hnet_to_solver_cond_id\": hnets_cond_ids[d_i][\"hnet->solver\"][\"hnet_root\"],\n",
    "                            \"hnet_init_theta\": None,\n",
    "                            \"hnet_prev_params\": hnet_root_prev_params,\n",
    "                            \"hnet_theta_out_target\": None,\n",
    "                            \"n_training_iters_solver\": config[\"n_training_iters_solver\"],\n",
    "                            \"n_training_iters_hnet\": config[\"n_training_iters_hnet\"],\n",
    "                        }\n",
    "                    ]\n",
    "                elif phase == \"hnet->hnet->solver\":\n",
    "                    cells = [\n",
    "                        {\n",
    "                            \"hnet\": hnet_child,\n",
    "                            \"solver\": solver_child,\n",
    "                            \"hnet_optim\": hnet_child_optim,\n",
    "                            \"hnet_to_hnet_cond_id\": None,\n",
    "                            \"hnet_to_solver_cond_id\": hnets_cond_ids[d_i][\"hnet->hnet->solver\"][\"hnet_child\"],\n",
    "                            \"hnet_init_theta\": hnet_root(cond_id=hnets_cond_ids[d_i][\"hnet->hnet->solver\"][\"hnet_root\"]),\n",
    "                            \"hnet_prev_params\": None, # TODO: would those also regularize the root hypernet?\n",
    "                            \"hnet_theta_out_target\": None,\n",
    "                            \"n_training_iters_solver\": config[\"n_training_iters_solver\"],\n",
    "                            \"n_training_iters_hnet\": 0,\n",
    "                        },\n",
    "                        {\n",
    "                            \"hnet\": hnet_root,\n",
    "                            \"solver\": solver_root,\n",
    "                            \"hnet_optim\": hnet_root_optim,\n",
    "                            \"hnet_to_hnet_cond_id\": hnets_cond_ids[d_i][\"hnet->hnet->solver\"][\"hnet_root\"],\n",
    "                            \"hnet_to_solver_cond_id\": hnets_cond_ids[d_i][\"hnet->solver\"][\"hnet_root\"],\n",
    "                            \"hnet_init_theta\": None,\n",
    "                            \"hnet_prev_params\": hnet_root_prev_params,\n",
    "                            \"hnet_theta_out_target\": None, # will get set during the train_cells() call\n",
    "                            # \"n_training_iters_solver\": config[\"n_training_iters_hnet\"], # root cell shouldn't take more steps on task than on hnet\n",
    "                            \"n_training_iters_solver\": 0, # root cell shouldn't take more steps on task than on hnet\n",
    "                            \"n_training_iters_hnet\": config[\"n_training_iters_hnet\"],\n",
    "                        }\n",
    "                    ]\n",
    "\n",
    "                validate_cells_training_inputs(X, y, cells, config)\n",
    "                stats = train_cells(X, y, cells, config, [])\n",
    "                # clear_output(wait=True)\n",
    "                if i % 3 == 2:\n",
    "                    print_stats(reversed(stats))\n",
    "                    print_metrics(\n",
    "                        datasets_for_eval, hnet_root, hnet_child, solver_root, solver_child,\n",
    "                        prefix=f\"[{p_i + 1}:{phase} | {d_i}/{len(data_handlers) - 1} | {epoch + 1}/{config['epochs']} | {i + 1}]\",\n",
    "                        skip_phases=[],\n",
    "                        wandb_run=wandb_run,\n",
    "                        additional_metrics=None\n",
    "                    )\n",
    "                    print(\"---\")\n",
    "    #             break\n",
    "    #         break\n",
    "    #     break\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(\n",
    "    datasets_for_eval, hnet_root, hnet_child, solver_root, solver_child,\n",
    "    prefix=f\"[{p_i + 1}:{phase} | {d_i}/{len(data_handlers) - 1} | {epoch + 1}/{config['epochs']} | {i + 1}]\",\n",
    "    skip_phases=[],\n",
    "    wandb_run=wandb_run,\n",
    "    additional_metrics=None\n",
    ")\n",
    "print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continual learning - full backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "hnet_root_prev_params = None\n",
    "log_step = 0\n",
    "phases = config[\"phases\"]\n",
    "\n",
    "for p_i, phase in enumerate(phases):\n",
    "    for d_i, data in enumerate(data_handlers):\n",
    "        # save parameters before solving the task for regularization against forgetting\n",
    "        hnet_root_prev_params = [p.detach().clone() for p_idx, p in enumerate(hnet_root.unconditional_params)]\n",
    "        for epoch in range(config[\"epochs\"]):\n",
    "            for i, (batch_size, X, y) in enumerate(data.train_iterator(config[\"data\"][\"batch_size\"])):\n",
    "                if config[\"max_minibatches_per_epoch\"] is not None and i > config[\"max_minibatches_per_epoch\"]:\n",
    "                    break\n",
    "\n",
    "                X = data.input_to_torch_tensor(X, config[\"device\"], mode=\"train\")\n",
    "                y = data.output_to_torch_tensor(y, config[\"device\"], mode=\"train\")\n",
    "\n",
    "                hnet_root_optim.zero_grad()\n",
    "                hnet_child_optim.zero_grad()\n",
    "\n",
    "                # select cond_ids for hypernets\n",
    "                hnet_root_cond_id = hnets_cond_ids[d_i][phase][\"hnet_root\"]\n",
    "                hnet_child_cond_id = hnets_cond_ids[d_i][phase][\"hnet_child\"]\n",
    "                # generate theta and predict\n",
    "                y_hat, params_solver = infer(X, phase, hnet_parent_cond_id=hnet_root_cond_id, hnet_child_cond_id=hnet_child_cond_id, hnet_parent=hnet_root, hnet_child=hnet_child, solver_parent=solver_root, solver_child=solver_child)\n",
    "                \n",
    "                # solvers' params regularization\n",
    "                loss_solver_params_reg = sum([p.norm(p=2) for p in params_solver]) / len(params_solver)\n",
    "                # task loss\n",
    "                loss_class = loss_fn(y_hat, y)\n",
    "                loss = loss_class + config[\"hnet\"][\"reg_alpha\"] * loss_solver_params_reg\n",
    "                loss.backward(retain_graph=True, create_graph=not config[\"hnet\"][\"detach_d_theta\"])\n",
    "                # gradient clipping\n",
    "                clip_grads([hnet_child, hnet_root], config[\"hnet\"][\"reg_clip_grads_max_norm\"], config[\"hnet\"][\"reg_clip_grads_max_value\"])\n",
    "                \n",
    "                # regularization against forgetting other contexts\n",
    "                loss_reg = config[\"hnet\"][\"reg_beta\"] * get_reg_loss(hnet_root, hnet_root_prev_params, hnet_root_optim, curr_cond_id=hnet_root_cond_id, lr=config[\"hnet\"][\"reg_lr\"], detach_d_theta=config[\"hnet\"][\"detach_d_theta\"])\n",
    "                loss_reg.backward()\n",
    "                clip_grads([hnet_child, hnet_root], config[\"hnet\"][\"reg_clip_grads_max_norm\"], config[\"hnet\"][\"reg_clip_grads_max_value\"])\n",
    "                \n",
    "                hnet_root_optim.step()\n",
    "                hnet_child_optim.step()\n",
    "                hnet_root_optim.zero_grad()\n",
    "                hnet_child_optim.zero_grad()\n",
    "\n",
    "                if i % 100 == 99:\n",
    "                    acc = (y_hat.argmax(dim=-1) == y.argmax(dim=-1)).float().mean() * 100.\n",
    "                    print_metrics(\n",
    "                        datasets_for_eval, hnet_root, hnet_child, solver_root, solver_child,\n",
    "                        prefix=f\"[{p_i + 1}:{phase} | {d_i}/{len(data_handlers) - 1} | {epoch + 1}/{config['epochs']} | {i + 1}]\",\n",
    "                        skip_phases=[],\n",
    "                        wandb_run=wandb_run, additional_metrics={\n",
    "                            \"loss_class\": loss_class.item(),\n",
    "                            \"acc_class\": acc,\n",
    "                            \"loss_solver_params_reg\": loss_solver_params_reg.item(),\n",
    "                            \"loss_reg\": loss_reg.item(),\n",
    "                        }\n",
    "                    )\n",
    "                    print(\"---\")\n",
    "                    log_step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training in multitask setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "hnet_root_prev_phase_params = None\n",
    "log_step = 0\n",
    "\n",
    "phases = [\"hnet->solver\", \"hnet->hnet->solver\", \"hnet->solver\", \"hnet->hnet->solver\", \"hnet->solver\", \"hnet->hnet->solver\"]\n",
    "for p_i, phase in enumerate(phases):\n",
    "    print(f\"\\n\\n.... Starting phase {phase} ...\")\n",
    "    if wandb_run is not None:\n",
    "        wandb_run.log({\"Phase\": wandb.Table(columns=[\"phase\", \"step\"], data=[[phase, log_step]])}) # log what phase the training is in\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        for i, ((_, m_X, m_y),(_, f_X, f_y)) in enumerate(zip(mnist.train_iterator(config[\"data\"][\"batch_size\"]), fmnist.train_iterator(config[\"data\"][\"batch_size\"]))):\n",
    "            if i > config[\"max_minibatches_per_epoch\"]:\n",
    "                break\n",
    "\n",
    "            # Mini-batch of MNIST samples\n",
    "            m_X = mnist.input_to_torch_tensor(m_X, config[\"device\"], mode=\"train\")\n",
    "            m_y = mnist.output_to_torch_tensor(m_y, config[\"device\"], mode=\"train\")\n",
    "            # Mini-batch of FashionMNIST samples\n",
    "            f_X = fmnist.input_to_torch_tensor(f_X, config[\"device\"], mode=\"train\")\n",
    "            f_y = fmnist.output_to_torch_tensor(f_y, config[\"device\"], mode=\"train\")\n",
    "\n",
    "            hnet_root_optim.zero_grad()\n",
    "            hnet_child_optim.zero_grad()\n",
    "\n",
    "            # MNIST ------------------------------------------------------------\n",
    "            if phase == \"hnet->solver\":\n",
    "                hnet_root_cond_id = 0\n",
    "                hnet_child_cond_id = None\n",
    "            elif phase == \"hnet->hnet->solver\":\n",
    "                hnet_root_cond_id = 2\n",
    "                hnet_child_cond_id = 0\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown phase {phase}\")\n",
    "            \n",
    "            y_hat, params_solver = infer(m_X, phase, hnet_root_cond_id=hnet_root_cond_id, hnet_child_cond_id=hnet_child_cond_id, hnet_root=hnet_root, hnet_child=hnet_child, solver_root=solver_root, solver_child=solver_child)\n",
    "            \n",
    "            # solvers' params regularization + task loss\n",
    "            m_loss_solver_params_reg = sum([p.norm(p=2) for p in params_solver]) / len(params_solver)\n",
    "            m_loss = loss_fn(y_hat, m_y.max(dim=1)[1]) + config[\"hnet\"][\"reg_alpha\"] * m_loss_solver_params_reg\n",
    "            m_acc = (y_hat.argmax(dim=-1) == m_y.argmax(dim=-1)).float().mean() * 100.\n",
    "            m_loss.backward()\n",
    "            if config[\"hnet\"][\"reg_clip_grads_max_norm\"] is not None:\n",
    "                clip_grads([hnet_child, hnet_root], config[\"hnet\"][\"reg_clip_grads_max_norm\"])\n",
    "            \n",
    "            # regularization against forgetting other contexts\n",
    "            m_loss_reg = config[\"hnet\"][\"reg_beta\"] * get_reg_loss(hnet_root, hnet_root_prev_phase_params, hnet_root_optim, curr_cond_id=hnet_root_cond_id, lr=config[\"hnet\"][\"reg_lr\"], detach_d_theta=False)\n",
    "            m_loss_reg.backward()\n",
    "            if config[\"hnet\"][\"reg_clip_grads_max_norm\"] is not None:\n",
    "                clip_grads([hnet_child, hnet_root], config[\"hnet\"][\"reg_clip_grads_max_norm\"])\n",
    "            \n",
    "            hnet_root_optim.step()\n",
    "            hnet_child_optim.step()\n",
    "            hnet_root_optim.zero_grad()\n",
    "            hnet_child_optim.zero_grad()\n",
    "\n",
    "\n",
    "            # FashionMNIST ------------------------------------------------------------\n",
    "            if phase == \"hnet->solver\":\n",
    "                hnet_root_cond_id = 1\n",
    "                hnet_child_cond_id = None\n",
    "            elif phase == \"hnet->hnet->solver\":\n",
    "                hnet_root_cond_id = 3\n",
    "                hnet_child_cond_id = 1\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown phase {phase}\")\n",
    "            \n",
    "            y_hat, params_solver = infer(f_X, phase, hnet_root_cond_id=hnet_root_cond_id, hnet_child_cond_id=hnet_child_cond_id, hnet_root=hnet_root, hnet_child=hnet_child, solver_root=solver_root, solver_child=solver_child)\n",
    "            \n",
    "            # solvers' params regularization + task loss\n",
    "            f_loss_solver_params_reg = sum([p.norm(p=2) for p in params_solver]) / len(params_solver)\n",
    "            f_loss = loss_fn(y_hat, f_y.max(dim=1)[1]) + config[\"hnet\"][\"reg_alpha\"] * f_loss_solver_params_reg\n",
    "            f_acc = (y_hat.argmax(dim=-1) == f_y.argmax(dim=-1)).float().mean() * 100.\n",
    "            f_loss.backward()\n",
    "            if config[\"hnet\"][\"reg_clip_grads_max_norm\"] is not None:\n",
    "                clip_grads([hnet_child, hnet_root], config[\"hnet\"][\"reg_clip_grads_max_norm\"])\n",
    "            \n",
    "            # regularization against forgetting other contexts\n",
    "            f_loss_reg = config[\"hnet\"][\"reg_beta\"] * get_reg_loss(hnet_root, hnet_root_prev_phase_params, hnet_root_optim, curr_cond_id=hnet_root_cond_id, lr=config[\"hnet\"][\"reg_lr\"], detach_d_theta=False)\n",
    "            f_loss_reg.backward()\n",
    "            if config[\"hnet\"][\"reg_clip_grads_max_norm\"] is not None:\n",
    "                clip_grads([hnet_child, hnet_root], config[\"hnet\"][\"reg_clip_grads_max_norm\"])\n",
    "            \n",
    "            hnet_root_optim.step()\n",
    "            hnet_child_optim.step()\n",
    "            hnet_root_optim.zero_grad()\n",
    "            hnet_child_optim.zero_grad()\n",
    "\n",
    "\n",
    "            if i % 100 == 99:\n",
    "                print_metrics(\n",
    "                    {\"MNIST\": (0, 2, 0, mnist), \"FashionMNIST\": (1, 3, 1, fmnist)},\n",
    "                    hnet_root, hnet_child, solver_root, solver_child,\n",
    "                    prefix=f\"[{phase} | {epoch}/{config['epochs']} | {i + 1}]\\nM: {m_loss_reg:.3f} F: {f_loss_reg:.3f}\",\n",
    "                    # skip_phases=[\"hnet->hnet->solver\"] if p_i == 0 and phase == \"hnet->solver\" else [],\n",
    "                    skip_phases=[],\n",
    "                    wandb_run=wandb_run, additional_metrics={\n",
    "                        \"m_loss_class\": m_loss.item() - config[\"hnet\"][\"reg_alpha\"] * m_loss_solver_params_reg.item(),\n",
    "                        \"f_loss_class\": f_loss.item() - config[\"hnet\"][\"reg_alpha\"] * f_loss_solver_params_reg.item(),\n",
    "                        \"m_acc_class\": m_acc,\n",
    "                        \"f_acc_class\": f_acc,\n",
    "                        \"m_loss_solver_params_reg\": m_loss_solver_params_reg.item(),\n",
    "                        \"f_loss_solver_params_reg\": f_loss_solver_params_reg.item(),\n",
    "                        \"m_loss_reg\": m_loss_reg.item(),\n",
    "                        \"f_loss_reg\": f_loss_reg.item(),\n",
    "                    }\n",
    "                )\n",
    "                print(\"---\")\n",
    "                log_step += 1\n",
    "    hnet_root_prev_phase_params = [p.detach().clone() for p_idx, p in enumerate(hnet_root.unconditional_params)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('vylet')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1aba0afea106d50199ec03ffaadaf3934529de3f3f9deaaa8fc5cd22ec9480e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
