{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from hypnettorch.data import FashionMNISTData, MNISTData\n",
    "from hypnettorch.mnets import LeNet, ResNet\n",
    "from hypnettorch.hnets import HMLP, StructuredHMLP, ChunkedHMLP\n",
    "\n",
    "from utils.data import get_mnist_data_loaders, get_emnist_data_loaders, randomize_targets, select_from_classes\n",
    "from utils.visualization import show_imgs, get_model_dot\n",
    "from utils.others import measure_alloc_mem, count_parameters\n",
    "from utils.timing import func_timer\n",
    "from utils.metrics import get_accuracy\n",
    "\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "torch.set_printoptions(precision=3, linewidth=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"epochs\": 2,\n",
    "    \"data\": {\n",
    "        \"batch_size\": 32,\n",
    "        \"data_dir\": \"data_tmp\",\n",
    "    },\n",
    "    \"solver\": {\n",
    "        \"use\": \"resnet\",\n",
    "        \"lenet\": {\n",
    "            \"arch\": \"mnist_large\",\n",
    "            \"num_classes\": 10,\n",
    "            \"no_weights\": True,\n",
    "        },\n",
    "        \"resnet\": {\n",
    "            \"n\": 5,\n",
    "            \"k\": 1,\n",
    "            \"use_bias\": True,\n",
    "            \"num_classes\": 10,\n",
    "            \"no_weights\": True,\n",
    "        },\n",
    "    },\n",
    "    \"hnet\": {\n",
    "        \"lr\": 1e-3,\n",
    "        \"reg_lr\": 1e-3,\n",
    "        \"model\": {\n",
    "            \"layers\": [128, 128],  \n",
    "        },\n",
    "        \"chunk_emb_size\": 16,\n",
    "        \"chunk_size\": 3500,\n",
    "        \"cond_in_size\": 32,\n",
    "        \"num_cond_embs\": 4,\n",
    "        \"cond_chunk_embs\": True,\n",
    "        \"reg_beta\": 0.005,\n",
    "        # \"reg_beta\": 0.0,\n",
    "    },\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    # \"device\": \"cpu\",\n",
    "}\n",
    "\n",
    "print(f\"... Running on {config['device']} ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = MNISTData(config[\"data\"][\"data_dir\"], use_one_hot=True, validation_size=0)\n",
    "fmnist = FashionMNISTData(config[\"data\"][\"data_dir\"], use_one_hot=True, validation_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target networks (solvers)\n",
    "if config[\"solver\"][\"use\"] == \"lenet\":\n",
    "    solver_child = LeNet(in_shape=mnist.in_shape, **config[\"solver\"][\"lenet\"]).to(config[\"device\"])\n",
    "    solver_root = LeNet(in_shape=mnist.in_shape, **config[\"solver\"][\"lenet\"]).to(config[\"device\"])\n",
    "elif config[\"solver\"][\"use\"] == \"resnet\":\n",
    "    solver_child = ResNet(in_shape=mnist.in_shape, **config[\"solver\"][\"resnet\"]).to(config[\"device\"])\n",
    "    solver_root = ResNet(in_shape=mnist.in_shape, **config[\"solver\"][\"resnet\"]).to(config[\"device\"])\n",
    "else:\n",
    "    raise ValueError(f\"Unknown solver: {config['solver']['use']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hnet = StructuredHMLP(\n",
    "#     solver.param_shapes,\n",
    "#     chunk_shapes=[[[16]], [[32]], [[64]], [[16, 16, 3, 3], [16]], [[32, 32, 3, 3], [32]], [[64, 64, 3, 3], [64]], [[10, 64], [10]]],\n",
    "#     num_per_chunk=[14, 12, 12, 6, 5, 5, 1],\n",
    "#     chunk_emb_sizes=32,\n",
    "#     hmlp_kwargs=config[\"hnet\"][\"model\"],\n",
    "#     assembly_fct=assembly_fct,\n",
    "#     uncond_in_size=0, cond_in_size=8, num_cond_embs=2).to(config[\"device\"]\n",
    "# )\n",
    "# \"\"\"\n",
    "# missing in chunk_shapes:\n",
    "# [16, 1, 3, 3] [16]\n",
    "# [32, 16, 3, 3] [32]\n",
    "# [64, 32, 3, 3] [64]\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hnet_child = ChunkedHMLP(\n",
    "    solver_child.param_shapes,\n",
    "    layers=config[\"hnet\"][\"model\"][\"layers\"],\n",
    "    chunk_size=config[\"hnet\"][\"chunk_size\"],\n",
    "    chunk_emb_size=config[\"hnet\"][\"chunk_emb_size\"],\n",
    "    cond_chunk_embs=config[\"hnet\"][\"cond_chunk_embs\"],\n",
    "    cond_in_size=config[\"hnet\"][\"cond_in_size\"],\n",
    "    num_cond_embs=config[\"hnet\"][\"num_cond_embs\"],\n",
    "    no_uncond_weights=True,\n",
    "    no_cond_weights=False,\n",
    ").to(config[\"device\"])\n",
    "\n",
    "hnet_root = ChunkedHMLP(\n",
    "    hnet_child.param_shapes,\n",
    "    layers=config[\"hnet\"][\"model\"][\"layers\"],\n",
    "    chunk_size=config[\"hnet\"][\"chunk_size\"],\n",
    "    chunk_emb_size=config[\"hnet\"][\"chunk_emb_size\"],\n",
    "    cond_chunk_embs=config[\"hnet\"][\"cond_chunk_embs\"],\n",
    "    cond_in_size=config[\"hnet\"][\"cond_in_size\"],\n",
    "    num_cond_embs=config[\"hnet\"][\"num_cond_embs\"],\n",
    "    no_uncond_weights=False,\n",
    "    no_cond_weights=False,\n",
    ").to(config[\"device\"])\n",
    "# hnet_root.apply_chunked_hyperfan_init(mnet=hnet_child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with chunking (hypernet -> hypernet -> target net)\n",
    "# def assembly_fct(list_of_chunks):\n",
    "#     assert len(list_of_chunks) == 107\n",
    "#     params_out = [\n",
    "#         list_of_chunks[0][0],\n",
    "#         list_of_chunks[1][0],\n",
    "#         list_of_chunks[2][0],\n",
    "#         list_of_chunks[0][1],\n",
    "#         list_of_chunks[2][1],\n",
    "#         list_of_chunks[1][1]\n",
    "#     ]\n",
    "#     to_concat = []\n",
    "#     for i in range(3, 52 + 3):\n",
    "#         to_concat.append(list_of_chunks[i][0])\n",
    "#     params_out.append(torch.cat(to_concat, dim=0))\n",
    "\n",
    "#     to_concat = []\n",
    "#     for i in range(52 + 3, 52 + 52 + 3):\n",
    "#         to_concat.append(list_of_chunks[i][0])\n",
    "#     params_out.append(torch.cat(to_concat, dim=0))\n",
    "    \n",
    "#     return params_out\n",
    "\n",
    "# hnet_root = StructuredHMLP(\n",
    "#     hnet_child.param_shapes,\n",
    "#     chunk_shapes=[[[8],[100]], [[100, 8], [100, 100]], [[420, 100]], [[420]]],\n",
    "#     num_per_chunk=[2, 1, 52, 52],\n",
    "#     chunk_emb_sizes=32,\n",
    "#     hmlp_kwargs=dict(layers=[100, 100]),\n",
    "#     assembly_fct=assembly_fct,\n",
    "#     uncond_in_size=0, cond_in_size=8, num_cond_embs=2).to(config[\"device\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(data, mnet, mnet_weights):\n",
    "    \"\"\"Compute the test accuracy for a given dataset\"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Process complete test set as one batch.\n",
    "        test_in = data.input_to_torch_tensor( \\\n",
    "            data.get_test_inputs(), config[\"device\"], mode='inference')\n",
    "        test_out = data.input_to_torch_tensor( \\\n",
    "            data.get_test_outputs(), config[\"device\"], mode='inference')\n",
    "        test_lbls = test_out.max(dim=1)[1]\n",
    "\n",
    "        if mnet_weights is not None:\n",
    "            logits = mnet(test_in, weights=mnet_weights)\n",
    "        else:\n",
    "            logits = mnet(test_in)\n",
    "        pred_lbls = logits.max(dim=1)[1]\n",
    "\n",
    "        acc = torch.sum(test_lbls == pred_lbls) / test_lbls.numel() * 100.\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_param_shapes(solver, params):\n",
    "    \"\"\"Correct the shapes of the parameters for the solver\"\"\"\n",
    "    params_solver = []\n",
    "    src_param_i = 0\n",
    "    src_param_start_idx = 0\n",
    "\n",
    "    for target_param_i, p_shape in enumerate(solver_root.param_shapes):\n",
    "        curr_available_src_params = params[src_param_i].flatten()[src_param_start_idx:].numel()\n",
    "        if curr_available_src_params >= math.prod(p_shape):\n",
    "            params_solver.append(params[src_param_i].flatten()[src_param_start_idx:src_param_start_idx + math.prod(p_shape)].view(p_shape))\n",
    "            src_param_start_idx += math.prod(p_shape)\n",
    "        else:\n",
    "            new_param = torch.zeros(math.prod(p_shape), device=config[\"device\"])\n",
    "            s, e = 0, 0\n",
    "\n",
    "            while math.prod(p_shape) > e:\n",
    "                curr_available_src_params = params[src_param_i].flatten().numel()\n",
    "                to_add = params[src_param_i].flatten()[src_param_start_idx:min(curr_available_src_params, src_param_start_idx + (math.prod(p_shape) - e))]\n",
    "                e = s + to_add.numel()\n",
    "                new_param[s:e] = to_add\n",
    "                s += to_add.numel()\n",
    "\n",
    "                if e < math.prod(p_shape):\n",
    "                    src_param_i += 1\n",
    "                    src_param_start_idx = 0\n",
    "                else:\n",
    "                    src_param_start_idx += to_add.numel()\n",
    "\n",
    "            params_solver.append(new_param.view(p_shape))\n",
    "    return params_solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_delta_theta(optimizer, lr, clip_delta=True, detach=False):\n",
    "    ret = []\n",
    "    for g in optimizer.param_groups:\n",
    "        for p in g[\"params\"]:\n",
    "            if p.grad is None:\n",
    "                ret.append(None)\n",
    "                continue\n",
    "            if detach:\n",
    "                ret.append(-lr * p.grad.detach().clone())\n",
    "            else:\n",
    "                ret.append(-lr * p.grad.clone())\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reg_loss_for_cond(hnet, hnet_optimizer, lr, reg_cond_id, detach_d_theta=False):\n",
    "    # prepare targets (theta for child nets predicted by previous hnet)\n",
    "    hnet_mode = hnet.training\n",
    "    hnet.eval()\n",
    "    with torch.no_grad():\n",
    "        theta_child_target = hnet(cond_id=reg_cond_id)\n",
    "    # detaching target below is important!\n",
    "    theta_child_target = torch.cat([p.detach().clone().view(-1) for p in theta_child_target])\n",
    "    hnet.train(mode=hnet_mode)\n",
    "    \n",
    "    d_theta = calc_delta_theta(hnet_optimizer, lr, detach=detach_d_theta)\n",
    "    theta_parent_for_pred = []\n",
    "    for _theta, _d_theta in zip(hnet.internal_params, d_theta):\n",
    "        if _d_theta is None:\n",
    "            theta_parent_for_pred.append(_theta)\n",
    "        else:\n",
    "            theta_parent_for_pred.append(_theta + _d_theta if detach_d_theta is False else _theta + _d_theta.detach())\n",
    "    theta_child_predicted = hnet(cond_id=reg_cond_id, weights=theta_parent_for_pred)\n",
    "    theta_child_predicted = torch.cat([p.view(-1) for p in theta_child_predicted])\n",
    "\n",
    "    return (theta_child_target - theta_child_predicted).pow(2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reg_loss(hnet, hnet_optimizer, curr_cond_id, lr=1e-3, clip_grads_max_norm=1., detach_d_theta=False):\n",
    "    if clip_grads_max_norm is not None:\n",
    "        torch.nn.utils.clip_grad_norm_(hnet.parameters(), clip_grads_max_norm)\n",
    "    reg_loss = 0\n",
    "    for c_i in range(hnet._num_cond_embs):\n",
    "        if curr_cond_id is not None and c_i == curr_cond_id:\n",
    "            continue\n",
    "        reg_loss += get_reg_loss_for_cond(hnet, hnet_optimizer, lr, c_i, detach_d_theta)\n",
    "    return reg_loss / (hnet._num_cond_embs - (curr_cond_id is not None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(datasets : dict, hnet_root, hnet_child, solver_root, solver_child, hnet_root_optimizer, loss_fn, prefix=\"\"):\n",
    "    print(prefix)\n",
    "    with torch.no_grad():\n",
    "        for data_name, (hnet_root_context_id_hnet_solver, hnet_root_context_id_hnet_hnet_solver, dataset, X, y) in datasets.items():\n",
    "            print(data_name)\n",
    "            print(\"    hnet->solver\")\n",
    "            hnet_root_context_id = hnet_root_context_id_hnet_solver\n",
    "            params_solver_root = hnet_root.forward(cond_id=hnet_root_context_id) # root hnet -> params root solver\n",
    "            y_hat_root = solver_root.forward(X, weights=correct_param_shapes(solver_root, params_solver_root))\n",
    "            loss_root = loss_fn(y_hat_root, y)\n",
    "            loss_reg = get_reg_loss(hnet_root, hnet_root_optimizer, curr_cond_id=hnet_root_context_id, lr=config[\"hnet\"][\"reg_lr\"], clip_grads_max_norm=1., detach_d_theta=False)\n",
    "            print(f\"        Loss: {loss_root.item():.3f} | Regularization loss: {loss_reg:.3f} | Accuracy: {calc_accuracy(dataset, solver_root, correct_param_shapes(solver_root, params_solver_root)):.3f}\")\n",
    "            \n",
    "            print(\"    hnet->hnet->solver\")\n",
    "            hnet_root_context_id = hnet_root_context_id_hnet_hnet_solver\n",
    "            params_hnet_child = hnet_root.forward(cond_id=hnet_root_context_id)[hnet_child._num_cond_embs:] # root hnet -> params child hnet (only the unconditional ones)\n",
    "            if len(hnet_child.conditional_param_shapes) is not hnet_child._num_cond_embs:\n",
    "                params_hnet_child = params_hnet_child[:-(len(hnet_child.conditional_param_shapes) - hnet_child._num_cond_embs)]\n",
    "            params_solver_child = hnet_child.forward(cond_id=0, weights=params_hnet_child) # child hnet -> params child solver\n",
    "            y_hat_child = solver_child.forward(X, weights=params_solver_child)\n",
    "            loss_child = loss_fn(y_hat_child, y)\n",
    "            loss_reg = get_reg_loss(hnet_root, hnet_root_optimizer, curr_cond_id=hnet_root_context_id, lr=config[\"hnet\"][\"reg_lr\"], clip_grads_max_norm=1., detach_d_theta=False)\n",
    "            print(f\"        Loss: {loss_child.item():.3f} | Regularization loss: {loss_reg:.3f} | Accuracy: {calc_accuracy(dataset, solver_child, params_solver_child):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hnet_root_optimizer = torch.optim.Adam(hnet_root.internal_params, lr=config[\"hnet\"][\"lr\"])\n",
    "hnet_child_optimizer = torch.optim.Adam(hnet_child.internal_params, lr=config[\"hnet\"][\"lr\"])\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "for phase in (\"hnet->solver\", \"hnet->hnet->solver\"):\n",
    "    print(f\"\\n.... Starting phase {phase} ...\")\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        for i, ((curr_batchsize, m_X, m_y),(_, f_X, f_y)) in enumerate(zip(mnist.train_iterator(config[\"data\"][\"batch_size\"]), fmnist.train_iterator(config[\"data\"][\"batch_size\"]))):\n",
    "            # Mini-batch of MNIST samples\n",
    "            m_X = mnist.input_to_torch_tensor(m_X, config[\"device\"], mode=\"train\")\n",
    "            m_y = mnist.output_to_torch_tensor(m_y, config[\"device\"], mode=\"train\")\n",
    "            # Mini-batch of FashionMNIST samples\n",
    "            f_X = fmnist.input_to_torch_tensor(f_X, config[\"device\"], mode=\"train\")\n",
    "            f_y = fmnist.output_to_torch_tensor(f_y, config[\"device\"], mode=\"train\")\n",
    "\n",
    "            hnet_root_optimizer.zero_grad()\n",
    "            hnet_child_optimizer.zero_grad()\n",
    "\n",
    "            # Compute MNIST loss\n",
    "            if phase == \"hnet->solver\":\n",
    "                hnet_root_context_id = 0\n",
    "                params_solver_root = hnet_root.forward(cond_id=hnet_root_context_id) # root hnet -> params root solver\n",
    "                y_hat = solver_root.forward(m_X, weights=correct_param_shapes(solver_root, params_solver_root))\n",
    "                m_loss = loss_fn(y_hat, m_y.max(dim=1)[1])\n",
    "            elif phase == \"hnet->hnet->solver\":\n",
    "                hnet_root_context_id = 2\n",
    "                params_hnet_child = hnet_root.forward(cond_id=hnet_root_context_id)[hnet_child._num_cond_embs:] # root hnet -> params child hnet (only the unconditional ones)\n",
    "                if len(hnet_child.conditional_param_shapes) is not hnet_child._num_cond_embs:\n",
    "                    params_hnet_child = params_hnet_child[:-(len(hnet_child.conditional_param_shapes) - hnet_child._num_cond_embs)]\n",
    "                params_solver_child = hnet_child.forward(cond_id=0, weights=params_hnet_child)\n",
    "                y_hat = solver_child.forward(m_X, weights=params_solver_child)\n",
    "                m_loss = loss_fn(y_hat, m_y.max(dim=1)[1])\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown phase {phase}\")\n",
    "            # regularization against forgetting other contexts\n",
    "            m_loss_reg = get_reg_loss(hnet_root, hnet_root_optimizer, curr_cond_id=hnet_root_context_id, lr=config[\"hnet\"][\"reg_lr\"], clip_grads_max_norm=1., detach_d_theta=False)\n",
    "        \n",
    "            # Compute FashionMNIST loss\n",
    "            if phase == \"hnet->solver\":\n",
    "                hnet_root_context_id = 1\n",
    "                params_solver_root = hnet_root.forward(cond_id=hnet_root_context_id) # root hnet -> params root solver\n",
    "                y_hat = solver_root.forward(f_X, weights=correct_param_shapes(solver_root, params_solver_root))\n",
    "                f_loss = loss_fn(y_hat, f_y.max(dim=1)[1])\n",
    "            elif phase == \"hnet->hnet->solver\":\n",
    "                hnet_root_context_id = 3\n",
    "                params_hnet_child = hnet_root.forward(cond_id=hnet_root_context_id)[hnet_child._num_cond_embs:] # root hnet -> params child hnet (only the unconditional ones)\n",
    "                if len(hnet_child.conditional_param_shapes) is not hnet_child._num_cond_embs:\n",
    "                    params_hnet_child = params_hnet_child[:-(len(hnet_child.conditional_param_shapes) - hnet_child._num_cond_embs)]\n",
    "                params_solver_child = hnet_child.forward(cond_id=1, weights=params_hnet_child)\n",
    "                y_hat = solver_child.forward(f_X, weights=params_solver_child)\n",
    "                f_loss = loss_fn(y_hat, f_y.max(dim=1)[1])\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown phase {phase}\")\n",
    "            # regularization against forgetting other contexts\n",
    "            f_loss_reg = get_reg_loss(hnet_root, hnet_root_optimizer, curr_cond_id=hnet_root_context_id, lr=config[\"hnet\"][\"reg_lr\"], clip_grads_max_norm=1., detach_d_theta=False)\n",
    "            \n",
    "            total_loss = m_loss + f_loss + 0.5 * config[\"hnet\"][\"reg_beta\"] * m_loss_reg + 0.5 * config[\"hnet\"][\"reg_beta\"] * f_loss_reg\n",
    "            total_loss.backward()\n",
    "            \n",
    "            hnet_root_optimizer.step()\n",
    "            hnet_child_optimizer.step()\n",
    "\n",
    "            if i % 100 == 99:\n",
    "                print_metrics({\"MNIST\": (0, 2, mnist, m_X, m_y.max(dim=1)[1]), \"FashionMNIST\": (1, 3, fmnist, f_X, f_y.max(dim=1)[1])}, hnet_root, hnet_child, solver_root, solver_child, hnet_root_optimizer, loss_fn, prefix=f\"[{phase} | {epoch}/{config['epochs']} | {i + 1}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics({\"MNIST\": (0, 2, mnist, m_X, m_y.max(dim=1)[1]), \"FashionMNIST\": (1, 3, fmnist, f_X, f_y.max(dim=1)[1])}, hnet_root, hnet_child, solver_root, solver_child, hnet_root_optimizer, loss_fn, prefix=f\"[{phase} | {epoch}/{config['epochs']} | {i + 1}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 98 mnist, 89 fmnist (hypernet -> target net)\n",
    "- 98 mnist, 88 fmnist (hypernet -> hypernet -> target net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('vylet')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1aba0afea106d50199ec03ffaadaf3934529de3f3f9deaaa8fc5cd22ec9480e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
