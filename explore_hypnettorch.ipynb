{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import gc\n",
    "import wandb\n",
    "from hypnettorch.data import FashionMNISTData, MNISTData\n",
    "from hypnettorch.data.special.split_mnist import get_split_mnist_handlers\n",
    "from hypnettorch.data.special.split_cifar import get_split_cifar_handlers\n",
    "from hypnettorch.mnets import LeNet, ResNet\n",
    "from hypnettorch.hnets import HMLP, StructuredHMLP, ChunkedHMLP\n",
    "\n",
    "from utils.data import get_mnist_data_loaders, get_emnist_data_loaders, randomize_targets, select_from_classes\n",
    "from utils.visualization import show_imgs, get_model_dot\n",
    "from utils.others import measure_alloc_mem, count_parameters\n",
    "from utils.timing import func_timer\n",
    "from utils.metrics import get_accuracy\n",
    "\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "torch.set_printoptions(precision=3, linewidth=180)\n",
    "%env \"WANDB_NOTEBOOK_NAME\" \"main.ipynb\"\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"epochs\": 2,\n",
    "    \"max_minibatches_per_epoch\": 1000,\n",
    "    \"phases\": [\"hnet->solver\", \"hnet->hnet->solver\", \"hnet->solver\", \"hnet->hnet->solver\", \"hnet->solver\", \"hnet->hnet->solver\"],\n",
    "    \"data\": {\n",
    "        # \"name\": \"mnist|fmnist\",\n",
    "        \"name\": \"splitmnist\",\n",
    "        \"batch_size\": 32,\n",
    "        \"data_dir\": \"data_tmp\",\n",
    "        \"num_tasks\": 5,\n",
    "        \"num_classes_per_task\": 2,\n",
    "        \"validation_size\": 0,\n",
    "    },\n",
    "    \"solver\": {\n",
    "        \"use\": \"lenet\",\n",
    "        \"lenet\": {\n",
    "            \"arch\": \"mnist_large\",\n",
    "            \"no_weights\": True,\n",
    "        },\n",
    "        \"resnet\": {\n",
    "            \"n\": 5,\n",
    "            \"k\": 1,\n",
    "            \"use_bias\": True,\n",
    "            \"no_weights\": True,\n",
    "        },\n",
    "    },\n",
    "    \"hnet\": {\n",
    "        \"lr\": 1e-3,\n",
    "        \"reg_lr\": 1e-3,\n",
    "        \"model\": {\n",
    "            \"layers\": [100, 100],\n",
    "            # \"layers\": [128, 128],\n",
    "            \"dropout_rate\": -1, # hmlp doesn't get images -> need to be added to resnet\n",
    "        },\n",
    "        \"chunk_emb_size\": 16,\n",
    "        \"chunk_size\": 4500,\n",
    "        \"cond_in_size\": 32,\n",
    "        \"cond_chunk_embs\": True,\n",
    "        \"reg_alpha\": 1e-2, # L2 regularization of solvers' parameters\n",
    "        \"reg_beta\": 8e-1, # regularization against forgetting other contexts (tasks)\n",
    "        \"detach_d_theta\": True,\n",
    "        \"reg_clip_grads_max_norm\": None,\n",
    "        \"reg_clip_grads_max_value\": 1.,\n",
    "    },\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"wandb_logging\": False,\n",
    "}\n",
    "\n",
    "print(f\"... Running on {config['device']} ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "if config[\"data\"][\"name\"] == \"mnist|fmnist\":\n",
    "    mnist = MNISTData(config[\"data\"][\"data_dir\"], use_one_hot=True, validation_size=config[\"data\"][\"validation_size\"])\n",
    "    fmnist = FashionMNISTData(config[\"data\"][\"data_dir\"], use_one_hot=True, validation_size=config[\"data\"][\"validation_size\"])\n",
    "    data_handlers = [mnist, fmnist]\n",
    "elif config[\"data\"][\"name\"] == \"splitmnist\":\n",
    "    data_handlers = get_split_mnist_handlers(config[\"data\"][\"data_dir\"], use_one_hot=True, num_tasks=config[\"data\"][\"num_tasks\"], num_classes_per_task=config[\"data\"][\"num_classes_per_task\"], validation_size=config[\"data\"][\"validation_size\"])\n",
    "elif config[\"data\"][\"name\"] == \"splitcifar\":\n",
    "    data_handlers = get_split_mnist_handlers(config[\"data\"][\"data_dir\"], use_one_hot=True, num_tasks=config[\"data\"][\"num_tasks\"], num_classes_per_task=config[\"data\"][\"num_classes_per_task\"], validation_size=config[\"data\"][\"validation_size\"])\n",
    "\n",
    "assert config[\"data\"][\"num_tasks\"] == len(data_handlers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "# target networks (solvers)\n",
    "if config[\"solver\"][\"use\"] == \"lenet\":\n",
    "    solver_child = LeNet(in_shape=data_handlers[0].in_shape, num_classes=config[\"data\"][\"num_classes_per_task\"], **config[\"solver\"][\"lenet\"]).to(config[\"device\"])\n",
    "    solver_root = LeNet(in_shape=data_handlers[0].in_shape, num_classes=config[\"data\"][\"num_classes_per_task\"], **config[\"solver\"][\"lenet\"]).to(config[\"device\"])\n",
    "elif config[\"solver\"][\"use\"] == \"resnet\":\n",
    "    solver_child = ResNet(in_shape=data_handlers[0].in_shape, num_classes=config[\"data\"][\"num_classes_per_task\"], **config[\"solver\"][\"resnet\"]).to(config[\"device\"])\n",
    "    solver_root = ResNet(in_shape=data_handlers[0].in_shape, num_classes=config[\"data\"][\"num_classes_per_task\"], **config[\"solver\"][\"resnet\"]).to(config[\"device\"])\n",
    "else:\n",
    "    raise ValueError(f\"Unknown solver: {config['solver']['use']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hnet = StructuredHMLP(\n",
    "#     solver.param_shapes,\n",
    "#     chunk_shapes=[[[16]], [[32]], [[64]], [[16, 16, 3, 3], [16]], [[32, 32, 3, 3], [32]], [[64, 64, 3, 3], [64]], [[10, 64], [10]]],\n",
    "#     num_per_chunk=[14, 12, 12, 6, 5, 5, 1],\n",
    "#     chunk_emb_sizes=32,\n",
    "#     hmlp_kwargs=config[\"hnet\"][\"model\"],\n",
    "#     assembly_fct=assembly_fct,\n",
    "#     uncond_in_size=0, cond_in_size=8, num_cond_embs=2).to(config[\"device\"]\n",
    "# )\n",
    "# \"\"\"\n",
    "# missing in chunk_shapes:\n",
    "# [16, 1, 3, 3] [16]\n",
    "# [32, 16, 3, 3] [32]\n",
    "# [64, 32, 3, 3] [64]\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "hnet_child = ChunkedHMLP(\n",
    "    solver_child.param_shapes,\n",
    "    layers=config[\"hnet\"][\"model\"][\"layers\"],\n",
    "    chunk_size=config[\"hnet\"][\"chunk_size\"],\n",
    "    chunk_emb_size=config[\"hnet\"][\"chunk_emb_size\"],\n",
    "    cond_chunk_embs=config[\"hnet\"][\"cond_chunk_embs\"],\n",
    "    cond_in_size=config[\"hnet\"][\"cond_in_size\"],\n",
    "    num_cond_embs=config[\"data\"][\"num_tasks\"] * 2, # num_tasks * 2 for child hypernetwork\n",
    "    no_uncond_weights=True,\n",
    "    no_cond_weights=False,\n",
    ").to(config[\"device\"])\n",
    "hnet_child_optim = torch.optim.Adam(hnet_child.conditional_params, lr=config[\"hnet\"][\"lr\"])\n",
    "\n",
    "hnet_root = ChunkedHMLP(\n",
    "    hnet_child.unconditional_param_shapes,\n",
    "    layers=config[\"hnet\"][\"model\"][\"layers\"],\n",
    "    dropout_rate=config[\"hnet\"][\"model\"][\"dropout_rate\"], # only for the root hypernetwork\n",
    "    chunk_size=config[\"hnet\"][\"chunk_size\"],\n",
    "    chunk_emb_size=config[\"hnet\"][\"chunk_emb_size\"],\n",
    "    cond_chunk_embs=config[\"hnet\"][\"cond_chunk_embs\"],\n",
    "    cond_in_size=config[\"hnet\"][\"cond_in_size\"],\n",
    "    num_cond_embs=config[\"data\"][\"num_tasks\"] * 2, # num_tasks * 2 for child hypernetwork\n",
    "    no_uncond_weights=False,\n",
    "    no_cond_weights=False,\n",
    ").to(config[\"device\"])\n",
    "hnet_root_optim = torch.optim.Adam(hnet_root.internal_params, lr=config[\"hnet\"][\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with chunking (hypernet -> hypernet -> target net)\n",
    "# def assembly_fct(list_of_chunks):\n",
    "#     assert len(list_of_chunks) == 107\n",
    "#     params_out = [\n",
    "#         list_of_chunks[0][0],\n",
    "#         list_of_chunks[1][0],\n",
    "#         list_of_chunks[2][0],\n",
    "#         list_of_chunks[0][1],\n",
    "#         list_of_chunks[2][1],\n",
    "#         list_of_chunks[1][1]\n",
    "#     ]\n",
    "#     to_concat = []\n",
    "#     for i in range(3, 52 + 3):\n",
    "#         to_concat.append(list_of_chunks[i][0])\n",
    "#     params_out.append(torch.cat(to_concat, dim=0))\n",
    "\n",
    "#     to_concat = []\n",
    "#     for i in range(52 + 3, 52 + 52 + 3):\n",
    "#         to_concat.append(list_of_chunks[i][0])\n",
    "#     params_out.append(torch.cat(to_concat, dim=0))\n",
    "    \n",
    "#     return params_out\n",
    "\n",
    "# hnet_root = StructuredHMLP(\n",
    "#     hnet_child.param_shapes,\n",
    "#     chunk_shapes=[[[8],[100]], [[100, 8], [100, 100]], [[420, 100]], [[420]]],\n",
    "#     num_per_chunk=[2, 1, 52, 52],\n",
    "#     chunk_emb_sizes=32,\n",
    "#     hmlp_kwargs=dict(layers=[100, 100]),\n",
    "#     assembly_fct=assembly_fct,\n",
    "#     uncond_in_size=0, cond_in_size=8, num_cond_embs=2).to(config[\"device\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calc_accuracy(X, y, solver, solver_weights, use_data_from=\"test\"):\n",
    "#     \"\"\"Compute the test accuracy for a given dataset (validation)\"\"\"\n",
    "#     # assert use_data_from == \"test\" or data.num_val_samples > 0, \"No validation data available.\"\n",
    "#     solver_train = solver.training\n",
    "#     solver.eval()\n",
    "#     acc = None\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         if use_data_from == \"validation\":\n",
    "#             raise NotImplementedError()\n",
    "#             # num_correct = 0\n",
    "\n",
    "#             # for batch_size, X, y, ids in data.val_iterator(config[\"data\"][\"batch_size\"], return_ids=True):\n",
    "#             #     X = data.input_to_torch_tensor(X, config[\"device\"], mode='inference')\n",
    "#             #     y = data.output_to_torch_tensor(y, config[\"device\"], mode='inference')\n",
    "#             #     y_hat = solver.forward(X, weights=solver_weights)\n",
    "#             #     num_correct += int(torch.sum(y_hat.argmax(dim=1) == y.argmax(dim=1)).detach().cpu())\n",
    "\n",
    "#             # acc = num_correct / data.num_val_samples * 100.\n",
    "#         elif use_data_from == \"test\":\n",
    "#                 if solver_weights is not None:\n",
    "#                     y_hat = solver(X, weights=solver_weights)\n",
    "#                 else:\n",
    "#                     y_hat = solver(X)\n",
    "\n",
    "#                 acc = (y_hat.argmax(dim=-1) == y.argmax(dim=-1)).float().mean() * 100.\n",
    "#         else:\n",
    "#             raise ValueError(\"Unknown data source (use 'test' or 'validation').\")\n",
    "\n",
    "#     solver.train(mode=solver_train)\n",
    "#     return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_param_shapes(solver, params):\n",
    "    \"\"\"Correct the shapes of the parameters for the solver\"\"\"\n",
    "    params_solver = []\n",
    "    src_param_i = 0\n",
    "    src_param_start_idx = 0\n",
    "\n",
    "    for target_param_i, p_shape in enumerate(solver_root.param_shapes):\n",
    "        curr_available_src_params = params[src_param_i].flatten()[src_param_start_idx:].numel()\n",
    "        if curr_available_src_params >= math.prod(p_shape):\n",
    "            params_solver.append(params[src_param_i].flatten()[src_param_start_idx:src_param_start_idx + math.prod(p_shape)].view(p_shape))\n",
    "            src_param_start_idx += math.prod(p_shape)\n",
    "        else:\n",
    "            new_param = torch.zeros(math.prod(p_shape), device=config[\"device\"])\n",
    "            s = 0\n",
    "\n",
    "            while math.prod(p_shape) > s:\n",
    "                curr_available_src_params = params[src_param_i].flatten().numel()\n",
    "                to_add = params[src_param_i].flatten()[src_param_start_idx:min(curr_available_src_params, src_param_start_idx + (math.prod(p_shape) - s))]\n",
    "                new_param[s:s + to_add.numel()] = to_add\n",
    "                s += to_add.numel()\n",
    "\n",
    "                if s < math.prod(p_shape):\n",
    "                    src_param_i += 1\n",
    "                    src_param_start_idx = 0\n",
    "                else:\n",
    "                    src_param_start_idx += to_add.numel()\n",
    "\n",
    "            params_solver.append(new_param.view(p_shape))\n",
    "    return params_solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_delta_theta(optimizer, lr, clip_delta=True, detach=False):\n",
    "    ret = []\n",
    "    for g in optimizer.param_groups:\n",
    "        for p in g[\"params\"]:\n",
    "            if p.grad is None:\n",
    "                ret.append(None)\n",
    "                continue\n",
    "            if detach:\n",
    "                ret.append(-lr * p.grad.detach().clone())\n",
    "            else:\n",
    "                ret.append(-lr * p.grad.clone())\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reg_loss_for_cond(hnet, hnet_prev_params, hnet_optimizer, lr, reg_cond_id, detach_d_theta=False):\n",
    "    # prepare targets (theta for child nets predicted by previous hnet)\n",
    "    hnet_mode = hnet.training\n",
    "    hnet.eval()\n",
    "    with torch.no_grad():\n",
    "        theta_child_target = hnet(cond_id=reg_cond_id, weights={\"uncond_weights\": hnet_prev_params} if hnet_prev_params is not None else None)\n",
    "    # detaching target below is important!\n",
    "    theta_child_target = torch.cat([p.detach().clone().view(-1) for p in theta_child_target])\n",
    "    hnet.train(mode=hnet_mode)\n",
    "    \n",
    "    d_theta = calc_delta_theta(hnet_optimizer, lr, detach=detach_d_theta)\n",
    "    theta_parent_for_pred = []\n",
    "    for _theta, _d_theta in zip(hnet.internal_params, d_theta):\n",
    "        if _d_theta is None:\n",
    "            theta_parent_for_pred.append(_theta)\n",
    "        else:\n",
    "            theta_parent_for_pred.append(_theta + _d_theta if detach_d_theta is False else _theta + _d_theta.detach())\n",
    "    theta_child_predicted = hnet(cond_id=reg_cond_id, weights=theta_parent_for_pred)\n",
    "    theta_child_predicted = torch.cat([p.view(-1) for p in theta_child_predicted])\n",
    "\n",
    "    return (theta_child_target - theta_child_predicted).pow(2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reg_loss(hnet, hnet_prev_params, hnet_optimizer, curr_cond_id, lr=1e-3, clip_grads_max_norm=1., detach_d_theta=False):\n",
    "    reg_loss = 0\n",
    "    for c_i in range(hnet._num_cond_embs):\n",
    "        if curr_cond_id is not None and c_i == curr_cond_id:\n",
    "            continue\n",
    "        reg_loss += get_reg_loss_for_cond(hnet, hnet_prev_params, hnet_optimizer, lr, c_i, detach_d_theta)\n",
    "    return reg_loss / (hnet._num_cond_embs - (curr_cond_id is not None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(X, scenario, hnet_root_cond_id, hnet_child_cond_id, hnet_root, hnet_child, solver_root, solver_child):\n",
    "    assert scenario != \"hnet->hnet->solver\" or hnet_child_cond_id is not None, f\"Scenario {scenario} requires hnet_child_cond_id to be set\"\n",
    "    \n",
    "    if scenario == \"hnet->solver\":\n",
    "        params_solver = hnet_root.forward(cond_id=hnet_root_cond_id) # root hnet -> params root solver\n",
    "        y_hat = solver_root.forward(X, weights=correct_param_shapes(solver_root, params_solver))\n",
    "    elif scenario == \"hnet->hnet->solver\":\n",
    "        params_hnet_child = hnet_root.forward(cond_id=hnet_root_cond_id) # root hnet -> params child hnet (only the unconditional ones)\n",
    "        params_solver = hnet_child.forward(cond_id=hnet_child_cond_id, weights=params_hnet_child)\n",
    "        y_hat = solver_child.forward(X, weights=params_solver)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown inference scenario {scenario}\")\n",
    "    return y_hat, params_solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(datasets : dict, hnet_root, hnet_child, solver_root, solver_child, prefix=\"\", skip_phases=[], wandb_run=None, additional_metrics=None):\n",
    "    # set the models to eval mode and return them to their original mode after\n",
    "    ms_modes = []\n",
    "    for m in [hnet_root, hnet_child, solver_root, solver_child]:\n",
    "        ms_modes.append([m, m.training])\n",
    "        m.eval()\n",
    "    wandb_metrics = {}\n",
    "    \n",
    "    print(prefix)\n",
    "    with torch.no_grad():\n",
    "        for data_name, (hnet_root_cond_id_hnet_solver, hnet_root_cond_id_hnet_hnet_solver, hnet_child_cond_id, dataset) in datasets.items():\n",
    "            print(data_name)\n",
    "\n",
    "            # prepare a test batch for calculating loss & getting solver params\n",
    "            X = dataset.input_to_torch_tensor(dataset.get_test_inputs(), config[\"device\"], mode=\"inference\")\n",
    "            y = dataset.output_to_torch_tensor(dataset.get_test_outputs(), config[\"device\"], mode=\"inference\")\n",
    "\n",
    "            hnet_solver_loss, hnet_solver_acc, hnet_hnet_solver_loss, hnet_hnet_solver_acc = np.nan, np.nan, np.nan, np.nan\n",
    "            if \"hnet->solver\" not in skip_phases:\n",
    "                print(\"    hnet->solver\")\n",
    "                y_hat, params_solver = infer(X, \"hnet->solver\", hnet_root_cond_id_hnet_solver, None, hnet_root, hnet_child, solver_root, solver_child)\n",
    "                hnet_solver_loss = F.cross_entropy(y_hat, y).item()\n",
    "                hnet_solver_acc = (y_hat.argmax(dim=-1) == y.argmax(dim=-1)).float().mean() * 100.\n",
    "                print(f\"        Loss: {hnet_solver_loss:.3f} | Accuracy: {hnet_solver_acc:.3f}\")\n",
    "            \n",
    "            if \"hnet->hnet->solver\" not in skip_phases:\n",
    "                print(\"    hnet->hnet->solver\")\n",
    "                y_hat, params_solver = infer(X, \"hnet->hnet->solver\", hnet_root_cond_id_hnet_hnet_solver, hnet_child_cond_id, hnet_root, hnet_child, solver_root, solver_child)\n",
    "                hnet_hnet_solver_loss = F.cross_entropy(y_hat, y).item()\n",
    "                hnet_hnet_solver_acc = (y_hat.argmax(dim=-1) == y.argmax(dim=-1)).float().mean() * 100.\n",
    "                print(f\"        Loss: {hnet_hnet_solver_loss:.3f} | Accuracy: {hnet_hnet_solver_acc:.3f}\")\n",
    "            \n",
    "            wandb_metrics[str(data_name)] = {\n",
    "                \"h->s loss\": hnet_solver_loss,\n",
    "                \"h->s acc\": hnet_solver_acc,\n",
    "                \"h->h->s loss\": hnet_hnet_solver_loss,\n",
    "                \"h->h->s acc\": hnet_hnet_solver_acc,\n",
    "            }\n",
    "    \n",
    "    if additional_metrics:\n",
    "        wandb_metrics.update(additional_metrics)\n",
    "        for n, v in additional_metrics.items():\n",
    "            print(f\"{n}: {v:.3f}\")\n",
    "\n",
    "    if wandb_run is not None:\n",
    "        wandb_run.log(wandb_metrics)\n",
    "    \n",
    "    for m, mode in ms_modes:\n",
    "        m.train(mode=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"wandb_logging\"]:\n",
    "    wandb_run = wandb.init(\n",
    "        project=\"Hypernets\", entity=\"johnny1188\", config=config, group=config[\"data\"][\"name\"],\n",
    "        tags=[], notes=f\"\"\n",
    "    )\n",
    "    wandb.watch((hnet_root, hnet_child, solver_root, solver_child), log=\"all\", log_freq=100)\n",
    "else:\n",
    "    wandb_run = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_grads(models, reg_clip_grads_max_norm, reg_clip_grads_max_value):\n",
    "    if reg_clip_grads_max_norm is not None and reg_clip_grads_max_value is not None:\n",
    "        print(\"Warning: both reg_clip_grads_max_norm and reg_clip_grads_max_value are set. Using reg_clip_grads_max_norm.\")\n",
    "    for m in models:\n",
    "        if reg_clip_grads_max_norm is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(m.parameters(), reg_clip_grads_max_norm)\n",
    "        elif reg_clip_grads_max_value is not None:\n",
    "            torch.nn.utils.clip_grad_value_(m.parameters(), reg_clip_grads_max_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training in multitask setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "hnet_root_prev_phase_params = None\n",
    "log_step = 0\n",
    "\n",
    "phases = [\"hnet->solver\", \"hnet->hnet->solver\", \"hnet->solver\", \"hnet->hnet->solver\", \"hnet->solver\", \"hnet->hnet->solver\"]\n",
    "for p_i, phase in enumerate(phases):\n",
    "    print(f\"\\n\\n.... Starting phase {phase} ...\")\n",
    "    if wandb_run is not None:\n",
    "        wandb_run.log({\"Phase\": wandb.Table(columns=[\"phase\", \"step\"], data=[[phase, log_step]])}) # log what phase the training is in\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        for i, ((_, m_X, m_y),(_, f_X, f_y)) in enumerate(zip(mnist.train_iterator(config[\"data\"][\"batch_size\"]), fmnist.train_iterator(config[\"data\"][\"batch_size\"]))):\n",
    "            if i > config[\"max_minibatches_per_epoch\"]:\n",
    "                break\n",
    "\n",
    "            # Mini-batch of MNIST samples\n",
    "            m_X = mnist.input_to_torch_tensor(m_X, config[\"device\"], mode=\"train\")\n",
    "            m_y = mnist.output_to_torch_tensor(m_y, config[\"device\"], mode=\"train\")\n",
    "            # Mini-batch of FashionMNIST samples\n",
    "            f_X = fmnist.input_to_torch_tensor(f_X, config[\"device\"], mode=\"train\")\n",
    "            f_y = fmnist.output_to_torch_tensor(f_y, config[\"device\"], mode=\"train\")\n",
    "\n",
    "            hnet_root_optim.zero_grad()\n",
    "            hnet_child_optim.zero_grad()\n",
    "\n",
    "            # MNIST ------------------------------------------------------------\n",
    "            if phase == \"hnet->solver\":\n",
    "                hnet_root_cond_id = 0\n",
    "                hnet_child_cond_id = None\n",
    "            elif phase == \"hnet->hnet->solver\":\n",
    "                hnet_root_cond_id = 2\n",
    "                hnet_child_cond_id = 0\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown phase {phase}\")\n",
    "            \n",
    "            y_hat, params_solver = infer(m_X, phase, hnet_root_cond_id=hnet_root_cond_id, hnet_child_cond_id=hnet_child_cond_id, hnet_root=hnet_root, hnet_child=hnet_child, solver_root=solver_root, solver_child=solver_child)\n",
    "            \n",
    "            # solvers' params regularization + task loss\n",
    "            m_loss_solver_params_reg = sum([p.norm(p=2) for p in params_solver]) / len(params_solver)\n",
    "            m_loss = loss_fn(y_hat, m_y.max(dim=1)[1]) + config[\"hnet\"][\"reg_alpha\"] * m_loss_solver_params_reg\n",
    "            m_acc = (y_hat.argmax(dim=-1) == m_y.argmax(dim=-1)).float().mean() * 100.\n",
    "            m_loss.backward()\n",
    "            if config[\"hnet\"][\"reg_clip_grads_max_norm\"] is not None:\n",
    "                clip_grads([hnet_child, hnet_root], config[\"hnet\"][\"reg_clip_grads_max_norm\"])\n",
    "            \n",
    "            # regularization against forgetting other contexts\n",
    "            m_loss_reg = config[\"hnet\"][\"reg_beta\"] * get_reg_loss(hnet_root, hnet_root_prev_phase_params, hnet_root_optim, curr_cond_id=hnet_root_cond_id, lr=config[\"hnet\"][\"reg_lr\"], detach_d_theta=False)\n",
    "            m_loss_reg.backward()\n",
    "            if config[\"hnet\"][\"reg_clip_grads_max_norm\"] is not None:\n",
    "                clip_grads([hnet_child, hnet_root], config[\"hnet\"][\"reg_clip_grads_max_norm\"])\n",
    "            \n",
    "            hnet_root_optim.step()\n",
    "            hnet_child_optim.step()\n",
    "            hnet_root_optim.zero_grad()\n",
    "            hnet_child_optim.zero_grad()\n",
    "\n",
    "\n",
    "            # FashionMNIST ------------------------------------------------------------\n",
    "            if phase == \"hnet->solver\":\n",
    "                hnet_root_cond_id = 1\n",
    "                hnet_child_cond_id = None\n",
    "            elif phase == \"hnet->hnet->solver\":\n",
    "                hnet_root_cond_id = 3\n",
    "                hnet_child_cond_id = 1\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown phase {phase}\")\n",
    "            \n",
    "            y_hat, params_solver = infer(f_X, phase, hnet_root_cond_id=hnet_root_cond_id, hnet_child_cond_id=hnet_child_cond_id, hnet_root=hnet_root, hnet_child=hnet_child, solver_root=solver_root, solver_child=solver_child)\n",
    "            \n",
    "            # solvers' params regularization + task loss\n",
    "            f_loss_solver_params_reg = sum([p.norm(p=2) for p in params_solver]) / len(params_solver)\n",
    "            f_loss = loss_fn(y_hat, f_y.max(dim=1)[1]) + config[\"hnet\"][\"reg_alpha\"] * f_loss_solver_params_reg\n",
    "            f_acc = (y_hat.argmax(dim=-1) == f_y.argmax(dim=-1)).float().mean() * 100.\n",
    "            f_loss.backward()\n",
    "            if config[\"hnet\"][\"reg_clip_grads_max_norm\"] is not None:\n",
    "                clip_grads([hnet_child, hnet_root], config[\"hnet\"][\"reg_clip_grads_max_norm\"])\n",
    "            \n",
    "            # regularization against forgetting other contexts\n",
    "            f_loss_reg = config[\"hnet\"][\"reg_beta\"] * get_reg_loss(hnet_root, hnet_root_prev_phase_params, hnet_root_optim, curr_cond_id=hnet_root_cond_id, lr=config[\"hnet\"][\"reg_lr\"], detach_d_theta=False)\n",
    "            f_loss_reg.backward()\n",
    "            if config[\"hnet\"][\"reg_clip_grads_max_norm\"] is not None:\n",
    "                clip_grads([hnet_child, hnet_root], config[\"hnet\"][\"reg_clip_grads_max_norm\"])\n",
    "            \n",
    "            hnet_root_optim.step()\n",
    "            hnet_child_optim.step()\n",
    "            hnet_root_optim.zero_grad()\n",
    "            hnet_child_optim.zero_grad()\n",
    "\n",
    "\n",
    "            if i % 100 == 99:\n",
    "                print_metrics(\n",
    "                    {\"MNIST\": (0, 2, 0, mnist), \"FashionMNIST\": (1, 3, 1, fmnist)},\n",
    "                    hnet_root, hnet_child, solver_root, solver_child,\n",
    "                    prefix=f\"[{phase} | {epoch}/{config['epochs']} | {i + 1}]\\nM: {m_loss_reg:.3f} F: {f_loss_reg:.3f}\",\n",
    "                    # skip_phases=[\"hnet->hnet->solver\"] if p_i == 0 and phase == \"hnet->solver\" else [],\n",
    "                    skip_phases=[],\n",
    "                    wandb_run=wandb_run, additional_metrics={\n",
    "                        \"m_loss_class\": m_loss.item() - config[\"hnet\"][\"reg_alpha\"] * m_loss_solver_params_reg.item(),\n",
    "                        \"f_loss_class\": f_loss.item() - config[\"hnet\"][\"reg_alpha\"] * f_loss_solver_params_reg.item(),\n",
    "                        \"m_acc_class\": m_acc,\n",
    "                        \"f_acc_class\": f_acc,\n",
    "                        \"m_loss_solver_params_reg\": m_loss_solver_params_reg.item(),\n",
    "                        \"f_loss_solver_params_reg\": f_loss_solver_params_reg.item(),\n",
    "                        \"m_loss_reg\": m_loss_reg.item(),\n",
    "                        \"f_loss_reg\": f_loss_reg.item(),\n",
    "                    }\n",
    "                )\n",
    "                print(\"---\")\n",
    "                log_step += 1\n",
    "    hnet_root_prev_phase_params = [p.detach().clone() for p_idx, p in enumerate(hnet_root.unconditional_params)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training in continual learning setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hnets_cond_ids = [\n",
    "    {\"hnet->solver\": {\"hnet_root\": task_i, \"hnet_child\": None}, \"hnet->hnet->solver\": {\"hnet_root\": task_i + len(data_handlers), \"hnet_child\": task_i}}\n",
    "    for task_i in range(len(data_handlers))\n",
    "]\n",
    "datasets_for_eval = {d_i: (cond_ids[\"hnet->solver\"][\"hnet_root\"], cond_ids[\"hnet->hnet->solver\"][\"hnet_root\"], cond_ids[\"hnet->hnet->solver\"][\"hnet_child\"], data_handlers[d_i]) for d_i, cond_ids in enumerate(hnets_cond_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "hnet_root_prev_params = None\n",
    "log_step = 0\n",
    "phases = config[\"phases\"]\n",
    "training_phases_table = wandb.Table(columns=[\"phase\", \"dataset_idx\", \"step\"], data=[])\n",
    "\n",
    "for p_i, phase in enumerate(phases):\n",
    "    print(f\"\\n\\n.... Starting phase {phase} ...\")\n",
    "    for d_i, data in enumerate(data_handlers):\n",
    "        hnet_root_prev_params = [p.detach().clone() for p_idx, p in enumerate(hnet_root.unconditional_params)]\n",
    "        \n",
    "        if wandb_run is not None:\n",
    "            training_phases_table.add_data(phase, d_i, log_step)\n",
    "            wandb_run.log({\"Phase\": training_phases_table}) # log what phase the training is in\n",
    "\n",
    "        for epoch in range(config[\"epochs\"]):\n",
    "            for i, (batch_size, X, y) in enumerate(data.train_iterator(config[\"data\"][\"batch_size\"])):\n",
    "                if i > config[\"max_minibatches_per_epoch\"]:\n",
    "                    break\n",
    "\n",
    "                X = data.input_to_torch_tensor(X, config[\"device\"], mode=\"train\")\n",
    "                y = data.output_to_torch_tensor(y, config[\"device\"], mode=\"train\")\n",
    "\n",
    "                hnet_root_optim.zero_grad()\n",
    "                hnet_child_optim.zero_grad()\n",
    "\n",
    "                # select cond_ids for hypernets\n",
    "                hnet_root_cond_id = hnets_cond_ids[d_i][phase][\"hnet_root\"]\n",
    "                hnet_child_cond_id = hnets_cond_ids[d_i][phase][\"hnet_child\"]\n",
    "                # generate theta and predict\n",
    "                y_hat, params_solver = infer(X, phase, hnet_root_cond_id=hnet_root_cond_id, hnet_child_cond_id=hnet_child_cond_id, hnet_root=hnet_root, hnet_child=hnet_child, solver_root=solver_root, solver_child=solver_child)\n",
    "                \n",
    "                # solvers' params regularization\n",
    "                loss_solver_params_reg = sum([p.norm(p=2) for p in params_solver]) / len(params_solver)\n",
    "                # task loss\n",
    "                loss_class = loss_fn(y_hat, y)\n",
    "                loss = loss_class + config[\"hnet\"][\"reg_alpha\"] * loss_solver_params_reg\n",
    "                loss.backward(retain_graph=True, create_graph=not config[\"hnet\"][\"detach_d_theta\"])\n",
    "                # gradient clipping\n",
    "                clip_grads([hnet_child, hnet_root], config[\"hnet\"][\"reg_clip_grads_max_norm\"], config[\"hnet\"][\"reg_clip_grads_max_value\"])\n",
    "                \n",
    "                # regularization against forgetting other contexts\n",
    "                loss_reg = config[\"hnet\"][\"reg_beta\"] * get_reg_loss(hnet_root, hnet_root_prev_params, hnet_root_optim, curr_cond_id=hnet_root_cond_id, lr=config[\"hnet\"][\"reg_lr\"], detach_d_theta=config[\"hnet\"][\"detach_d_theta\"])\n",
    "                loss_reg.backward()\n",
    "                clip_grads([hnet_child, hnet_root], config[\"hnet\"][\"reg_clip_grads_max_norm\"], config[\"hnet\"][\"reg_clip_grads_max_value\"])\n",
    "                \n",
    "                hnet_root_optim.step()\n",
    "                hnet_child_optim.step()\n",
    "                hnet_root_optim.zero_grad()\n",
    "                hnet_child_optim.zero_grad()\n",
    "\n",
    "                if i % 100 == 99:\n",
    "                    acc = (y_hat.argmax(dim=-1) == y.argmax(dim=-1)).float().mean() * 100.\n",
    "                    print_metrics(\n",
    "                        datasets_for_eval, hnet_root, hnet_child, solver_root, solver_child,\n",
    "                        prefix=f\"[{p_i + 1}:{phase} | {d_i}/{len(data_handlers)} | {epoch + 1}/{config['epochs']} | {i + 1}]\",\n",
    "                        skip_phases=[],\n",
    "                        wandb_run=wandb_run, additional_metrics={\n",
    "                            \"loss_class\": loss_class.item(),\n",
    "                            \"acc_class\": acc,\n",
    "                            \"loss_solver_params_reg\": loss_solver_params_reg.item(),\n",
    "                            \"loss_reg\": loss_reg.item(),\n",
    "                        }\n",
    "                    )\n",
    "                    print(\"---\")\n",
    "                    log_step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_y = mnist.output_to_torch_tensor(m_y, config[\"device\"], mode=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat, params_solver = infer(m_X, \"hnet->hnet->solver\", hnet_root_cond_id=2, hnet_child_cond_id=0, hnet_root=hnet_root, hnet_child=hnet_child, solver_root=solver_root, solver_child=solver_child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(\n",
    "    {\"MNIST\": (0, 2, 0, mnist)},\n",
    "    hnet_root, hnet_child, solver_root, solver_child,\n",
    "    prefix=f\"[{phase} | {epoch}/{config['epochs']} | {i + 1}]\\nM: {m_loss_reg:.3f} F: {f_loss_reg:.3f}\",\n",
    "    skip_phases=[\"hnet->hnet->solver\"] if p_i == 0 and phase == \"hnet->solver\" else [],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_solver = hnet_root(cond_id=0)\n",
    "y_h = solver_root.forward(test_in[:32], weights=correct_param_shapes(solver_root, p_solver))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_in = mnist.input_to_torch_tensor( \\\n",
    "#     mnist.get_test_inputs(), config[\"device\"], mode='inference')\n",
    "# test_out = mnist.input_to_torch_tensor( \\\n",
    "#     mnist.get_test_outputs(), config[\"device\"], mode='inference')\n",
    "# test_lbls = test_out.max(dim=1)[1]\n",
    "\n",
    "s, e = 6000,6500\n",
    "y_hat, params_solver = infer(test_in[s:e], \"hnet->solver\", hnet_root_cond_id=0, hnet_child_cond_id=None, hnet_root=hnet_root, hnet_child=hnet_child, solver_root=solver_root, solver_child=solver_child)\n",
    "print(\"MNIST test accuracy:\", (y_hat.max(dim=1)[1] == test_out[s:e].max(dim=1)[1]).float().mean())\n",
    "\n",
    "# logits = solver_root(test_in, weights=correct_param_shapes(solver_root, params_solver))\n",
    "# pred_lbls = logits.max(dim=1)[1]\n",
    "\n",
    "# torch.sum(test_lbls == pred_lbls) / test_lbls.numel() * 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 98 mnist, 89 fmnist (hypernet -> target net)\n",
    "- 98 mnist, 88 fmnist (hypernet -> hypernet -> target net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('vylet')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1aba0afea106d50199ec03ffaadaf3934529de3f3f9deaaa8fc5cd22ec9480e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
