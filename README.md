# Hierarchical Hypernetworks

[Intro](#intro) - [Usage](#usage) - [Results](#results)

---

## Intro
It is an established view that memories in the brain are not encoded and stored in isolation, but rather are interleaved with previously acquired related information. Moreover, in recent studies, sleep has been shown to not only integrate new information into established cortical memory networks, but also extract meaning from experience, and serve for developing insights. This points to the interaction between memory and the development of higher order, conceptual mental representations that support decision-making and other cognitive skills. **In this project, I focus on the problem of continual knowledge acquisition since I believe it underpins most of the other aspects of intelligence like commonsense reasoning, planning, and creativity**.

Specifically, I explore **Hierarchical Hypernetworks** - growing, self-generating memory systems capable of lifelong learning that recall and compress themselves in response to incoming data. Main question of interest is the limiting factors of recursively generated hypernetworks for encoding memories and learning new skills.

Hypernetworks, as introduced in [Ha et al. (2016)](https://arxiv.org/abs/1609.09106), are deep neural networks that instead of solving specified tasks directly, output parameters of another target network which is then used for the task at hand. The motivation lies in the ability to condition the parameters of the target network on a specific task and hence introduce something akin to fast weights [(Schmidhuber (1992))](https://ieeexplore.ieee.org/document/6796337), although with much greater power of complete one-step rewriting. Another possible advantage is compression.

*Note: This is a work in progress and the repository is not actively updated.*

## Usage
*Work in progress*

## Results
*Work in progress*
